[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/2022-01-10-sklearn-2.html",
    "href": "posts/2022-01-10-sklearn-2.html",
    "title": "Sklearn - Adult Income Classificat",
    "section": "",
    "text": "Repeating content from http://appliedprogramming.net/machine-learning/adult-income-classification.html\n\nimport os\nimport urllib\nfrom urllib.request import urlretrieve\nfrom pathlib import Path\n\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom time import time\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import roc_auc_score, classification_report\n\n\n# download the data\nbase_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult'\nfilename = 'adult.data'\nurlretrieve(f'{base_url}/{filename}', f'{filename}')\n\n('adult.data', &lt;http.client.HTTPMessage at 0x7fe9c7480e10&gt;)\n\n\n\ncols = [\"Age\", \"Work-Class\", \"fnlwgt\", \"Education\", \"Education-Num\", \"Marital-Status\", \"Occupation\", \n        \"Relationship\", \"Race\", \"Sex\", \"Capital-gain\", \"Capital-loss\", \"Hours-per-week\", \"Native-Country\", \"Earnings-Raw\"]\ndf = pd.read_csv(filename, header=None, names=cols)\nprint(f'df.shape - {df.shape}')\ndf.head(3)\n\ndf.shape - (32561, 15)\n\n\n\n\n\n\n\n\n\nAge\nWork-Class\nfnlwgt\nEducation\nEducation-Num\nMarital-Status\nOccupation\nRelationship\nRace\nSex\nCapital-gain\nCapital-loss\nHours-per-week\nNative-Country\nEarnings-Raw\n\n\n\n\n0\n39\nState-gov\n77516\nBachelors\n13\nNever-married\nAdm-clerical\nNot-in-family\nWhite\nMale\n2174\n0\n40\nUnited-States\n&lt;=50K\n\n\n1\n50\nSelf-emp-not-inc\n83311\nBachelors\n13\nMarried-civ-spouse\nExec-managerial\nHusband\nWhite\nMale\n0\n0\n13\nUnited-States\n&lt;=50K\n\n\n2\n38\nPrivate\n215646\nHS-grad\n9\nDivorced\nHandlers-cleaners\nNot-in-family\nWhite\nMale\n0\n0\n40\nUnited-States\n&lt;=50K\n\n\n\n\n\n\n\n\ndf.replace([' &lt;=50K', ' &gt;50K'], [0, 1], inplace=True)\ndf.dropna(how='all', inplace=True)\n\n\nSelecting K Best features\n\nx_cols = [\"Age\", \"Education-Num\", \"Capital-gain\", \"Capital-loss\", \"Hours-per-week\"]\nx_train_np = df[x_cols].values\ny_train_np = df['Earnings-Raw'].values\n\nprint(f'x_train_np.shape - {x_train_np.shape}. y_train_np.shape - {y_train_np.shape}')\n\nx_train_np.shape - (32561, 5). y_train_np.shape - (32561,)\n\n\nNow we create our transfomer using the chi2 function and a SelectKBest transformer\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\ntransformer = SelectKBest(score_func=chi2, k=3)\n\nThis will create a new dataset with reduced number of features\n\nx_chi2_train_np = transformer.fit_transform(x_train_np, y_train_np)\nprint(f'x_chi2_train_np.shape - {x_chi2_train_np.shape}')\nprint(f'chi square scores - {transformer.scores_}')\nprint(f'selected columns are {np.argsort(transformer.scores_)[-3:]}')\n\nx_chi2_train_np.shape - (32561, 3)\nchi square scores - [0.2340371  0.33515395 0.22332882 0.15052631 0.22968907]\nselected columns are [4 0 1]\n\n\nWe could also implement other correlations such as Pearsons correlation coefficient. As the default scipy pearsonr function accepts only a one dimensional array, we will create a wrapper around it.\n\nfrom scipy.stats import pearsonr\n\ndef multivariate_pearsonr(x, y):\n    scores, pvalues = [], []\n    for column in range(x.shape[1]):\n        cur_score, cur_p = pearsonr(x[:, column], y)\n        scores.append(abs(cur_score))\n        pvalues.append(cur_p)\n    return (np.array(scores),np.array(pvalues))\n\nThe pearson value could be between -1 and 1.\n\ntransformer = SelectKBest(score_func=multivariate_pearsonr, k=3)\nx_pearson_train_np = transformer.fit_transform(x_train_np, y_train_np)\nprint(transformer.scores_)\nprint(f'selected columns are {np.argsort(transformer.scores_)[-3:]}')\n\n[0.2340371  0.33515395 0.22332882 0.15052631 0.22968907]\nselected columns are [4 0 1]\n\n\nLet us now fit the classifier\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nclf = DecisionTreeClassifier()\nscores_chi2 = cross_val_score(clf, x_chi2_train_np, y_train_np, scoring='accuracy')\nscores_pearson = cross_val_score(clf, x_pearson_train_np, y_train_np, scoring='accuracy')\n\nprint(\"Chi2 performance: {0:.3f}\".format(scores_chi2.mean()))\nprint(\"Pearson performance: {0:.3f}\".format(scores_pearson.mean()))\n\nChi2 performance: 0.773\nPearson performance: 0.773\n\n\nUnlike in the blog, we got the same output for selectkbest. Hence the performance is same for both\n\n\nPrincipal Component Analysis\n\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=5)\nxd = pca.fit_transform(x_train_np)\nprint(f'xd.shape - {xd.shape}')\n\nnp.set_printoptions(precision=3, suppress=True)\npca.explained_variance_ratio_\n\nxd.shape - (32561, 5)\n\n\narray([0.997, 0.003, 0.   , 0.   , 0.   ])\n\n\nThis shows us that the first feature accounts for 99.7 percent of the variance in the dataset\n\n# fit with full dataset\nclf = DecisionTreeClassifier(random_state=14)\noriginal_scores = cross_val_score(clf, x_train_np, y_train_np, scoring='accuracy')\n\nprint(\"The average score from the original dataset is {:.4f}\".format(np.mean(original_scores)))\n\n# fit with reduced features \n\nThe average score from the original dataset is 0.8131\n\n\n\nx_train_np.shape\n\n(32561, 5)\n\n\nLets convert the categorical features into numeric values\n\ncategorical_features = []\nfor col in df.columns:\n    if df[col].dtype != np.int64:\n        categorical_features.append(col)\n        \ncategorical_features\n\n['Work-Class',\n 'Education',\n 'Marital-Status',\n 'Occupation',\n 'Relationship',\n 'Race',\n 'Sex',\n 'Native-Country']"
  },
  {
    "objectID": "posts/2023-04-19-text2sql-resdsql.html",
    "href": "posts/2023-04-19-text2sql-resdsql.html",
    "title": "Purpose",
    "section": "",
    "text": "We will attempt to understand the ResdSQL paper and its implementation in github\nSo the two main features of this paper are - Ranking-enhanced Encoding : Instead of using the whole schema, the encoder is injected with the most relevant schema items. we train an additional cross-encoder to classify the tables and columns simultaneously based on the input question, and then rank and filter them according to the classification probabilities to form a ranked schema sequence - Skeleton-first Decoding : The decoder first generates the skeleton (SQL keywords) and then the actual SQL query. Since skeleton parsing is much easier than SQL parsing, the first generated skeleton could implicitly guide the subsequent SQL parsing via the masked self-attention mechanism in the decoder."
  },
  {
    "objectID": "posts/2023-04-19-text2sql-resdsql.html#implementation",
    "href": "posts/2023-04-19-text2sql-resdsql.html#implementation",
    "title": "Purpose",
    "section": "Implementation",
    "text": "Implementation\nWe will identify the steps performed by the scripts in github repository.\n1. Run the contents of preprocess.sh\nMinor changes (remove the –db_path parameter to use the default)\npython preprocessing.py \\\n    --mode \"train\" \\\n    --table_path \"./data/spider/tables.json\" \\\n    --input_dataset_path \"./data/spider/train_spider.json\" \\\n    --output_dataset_path \"./data/preprocessed_data/preprocessed_train_spider.json\" \\\n    --target_type \"sql\"\n\npython preprocessing.py \\\n    --mode \"eval\" \\\n    --table_path \"./data/spider/tables.json\" \\\n    --input_dataset_path \"./data/spider/dev.json\" \\\n    --output_dataset_path \"./data/preprocessed_data/preprocessed_dev.json\" \\\n    --target_type \"sql\"\nThe preprocessing adds the following to the dataset and saves it in a new json file. - norm_sql : normalized SQL query - sql_skeleton : SQL skeleton - nat_sql : nat_sql if it exists (for train and dev, not test) - norm_nat_sql : normalized nat_sql if it exists - natsql_skeleton : nat_sql skeleton - pk : primary keys - fk : foreign keys - db_schema: database schema with original and semantic table and column names - db_contents: column value matches. See the query ‘What are the names of the heads who are born outside the California state’ in the generated file preprocessed_train_spider.json"
  },
  {
    "objectID": "posts/2022-05-13-timeseries-1.html",
    "href": "posts/2022-05-13-timeseries-1.html",
    "title": "Passenger dataset",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom random import gauss\nfrom pandas.plotting import autocorrelation_plot\nimport warnings\nimport itertools\nfrom random import random\n\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\nfrom statsmodels.tsa.ar_model import AR\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.stattools import adfuller\n\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\nimport seaborn as sns\n\n\n# general settings\nclass CFG:\n    data_folder = '../input/tsdata-1/'\n    img_dim1 = 20\n    img_dim2 = 10\n    \nplt.rcParams.update({'figure.figsize': (CFG.img_dim1, CFG.img_dim2)})\n\ncan be downloaded from https://www.kaggle.com/datasets/chirag19/air-passengers\n\nseries = pd.read_csv('AirPassengers.csv')\nseries['date'] = pd.to_datetime(series['Month'])\nseries.set_index('date', inplace=True)\nseries.head(3)\n\n\n\n\n\n\n\n\nMonth\n#Passengers\n\n\ndate\n\n\n\n\n\n\n1949-01-01\n1949-01\n112\n\n\n1949-02-01\n1949-02\n118\n\n\n1949-03-01\n1949-03\n132\n\n\n\n\n\n\n\n\n!ls -lrt\n\ntotal 9576\n-rw-r--r--  1 achinta  staff  3162384 May 13 20:17 2020-02-20-test.ipynb\n-rw-r--r--  1 achinta  staff     5385 May 13 20:17 2021-08-11-ar-net.ipynb\n-rw-r--r--  1 achinta  staff  1424885 May 13 20:17 2021-11-11-linear-algebra-svd.ipynb\n-rw-r--r--  1 achinta  staff    25836 May 13 20:17 2021-11-22-01-the-machine-learning-landscape.ipynb\n-rw-r--r--  1 achinta  staff   139119 May 13 20:17 2021-11-28-sklearn-intro.ipynb\n-rw-r--r--  1 achinta  staff     2844 May 13 20:17 2021-12-23-LA-column-spaces.ipynb\n-rw-r--r--  1 achinta  staff    37424 May 13 20:17 2021-12-28-variational-inference.ipynb\n-rw-r--r--  1 achinta  staff     3807 May 13 20:17 2022-01-06-linear-regression.ipynb\n-rw-r--r--  1 achinta  staff    14865 May 13 20:17 2022-01-10-sklearn-2.ipynb\n-rw-r--r--  1 achinta  staff    11212 May 13 20:17 2022-01-16-movie-recommender-pytorch.ipynb\n-rw-r--r--  1 achinta  staff    37422 May 13 20:17 2022-03-07-product-recomendation.ipynb\n-rw-r--r--  1 achinta  staff      771 May 13 20:17 README.md\n-rw-r--r--  1 achinta  staff     6436 May 13 20:17 algos.ipynb\ndrwxr-xr-x  4 achinta  staff      128 May 13 20:17 ghtop_images\ndrwxr-xr-x  3 achinta  staff       96 May 13 20:17 my_icons\n-rw-r--r--  1 achinta  staff     2049 May 13 20:46 2022-05-13-timeseries-1.ipynb\n\n\n\n!pip install redis\n\nCollecting redis\n  Using cached redis-4.3.1-py3-none-any.whl (241 kB)\nRequirement already satisfied: packaging&gt;=20.4 in /Users/achinta/miniforge3/envs/ml/lib/python3.9/site-packages (from redis) (21.3)\nCollecting deprecated&gt;=1.2.3\n  Using cached Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\nCollecting async-timeout&gt;=4.0.2\n  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\nCollecting wrapt&lt;2,&gt;=1.10\n  Downloading wrapt-1.14.1-cp39-cp39-macosx_11_0_arm64.whl (35 kB)\nRequirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /Users/achinta/miniforge3/envs/ml/lib/python3.9/site-packages (from packaging&gt;=20.4-&gt;redis) (3.0.9)\nInstalling collected packages: wrapt, async-timeout, deprecated, redis\nSuccessfully installed async-timeout-4.0.2 deprecated-1.2.13 redis-4.3.1 wrapt-1.14.1\n\n\n\nimport redis\n\n\nimport redis\n\nr = redis.Redis()\nr.set('foo', 'bar')\n\nConnectionError: Connection closed by server."
  },
  {
    "objectID": "posts/2023-02-19-deepseed.html",
    "href": "posts/2023-02-19-deepseed.html",
    "title": "notes",
    "section": "",
    "text": "This is notes from the ZeRO - Memory Optimization paper\nData Parallelism (DP) and Model Parallelism (MP) are two ways to parallelize training.\n\n\n\n\n\n\n\n\n\nData Parallel\nModel Parallel\n\n\n\n\nSplit\nData is split across multiple GPUs and each GPU contains a copy of the modes\nWe split the model across multiple GPUs. Each GPU contains a copy of the data.\n\n\nGradient Update\nGradients are averaged across GPUs and the model is updated.\n\n\n\nMemory Efficiency\nLow. Does not reduce the memory occupied by the model. Runs out of memory for models with more than 1.4B parameters.\nHigh\n\n\nCompute/communication efficiency\nHigh.\nLow. Works well within a single node, due to high inter-gpu communication\n\n\n\n\n\n\nimage from paper"
  },
  {
    "objectID": "posts/2021-11-28-sklearn-intro.html",
    "href": "posts/2021-11-28-sklearn-intro.html",
    "title": "sklearn intro",
    "section": "",
    "text": "Repeating the contents of http://appliedprogramming.net/machine-learning/introduction.html\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn import datasets\n\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
    "objectID": "posts/2021-11-28-sklearn-intro.html#clustering",
    "href": "posts/2021-11-28-sklearn-intro.html#clustering",
    "title": "sklearn intro",
    "section": "Clustering",
    "text": "Clustering\n\nx_train, x_test, y_train, y_test = train_test_split(x_iris, y_iris, test_size=0.25)\n\nscaler = StandardScaler().fit(x_train)\nx_train = scaler.transform(x_train)\nx_test = scaler.transform(x_test)\n\n\nfrom sklearn import cluster\nclf_sepal = cluster.KMeans(init='k-means++', n_clusters=3, random_state=33)\nclf_sepal.fit(x_train[:, 0:2])\n\nKMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n    n_clusters=3, n_init=10, n_jobs=1, precompute_distances='auto',\n    random_state=33, tol=0.0001, verbose=0)\n\n\nPrint the labels assigned\n\nprint(clf_sepal.labels_)\n\n[1 0 0 1 1 2 0 0 0 2 0 1 2 1 0 0 2 1 2 2 0 0 0 2 2 1 0 1 2 0 1 2 2 2 1 0 1\n 0 2 1 2 2 0 0 0 0 2 1 2 1 2 2 2 0 1 0 0 0 2 0 0 1 2 1 2 0 0 0 2 0 2 2 2 0\n 0 2 0 1 1 2 0 2 1 1 2 2 1 0 1 0 0 0 2 1 0 2 0 1 2 1 1 2 0 0 0 0 0 1 2 2 0\n 1]\n\n\n\ncolormarkers = [['red', 's'], ['greenyellow', 'o'], ['blue', 'x']]\nstep = 0.01\nmargin = 0.1\nsl_min, sl_max = x_train[:, 0].min() - margin, x_train[:, 0].max() + margin\nsw_min, sw_max = x_train[:, 1].min() - margin, x_train[:, 1].max() + margin\n\n# meshgrid returns two vectors one for x and y of length xy, which reprsent coordinates for the xy points. \nsl, sw = np.meshgrid(np.arange(sl_min, sl_max, step),\n                     np.arange(sw_min, sw_max, step))\nzs = clf_sepal.predict(np.c_[sl.ravel(), sw.ravel()]).reshape(sl.shape)\ncentroids_s = clf_sepal.cluster_centers_\n\nDisplay the data points and the calculated regions\n\nfig, ax = plt.subplots(figsize=(6,6))\nax.imshow(zs, interpolation='nearest',  extent=(sl.min(), sl.max(), sw.min(), sw.max()))\nax.set_xlim(sl_min, sl_max)\nax.set_ylim(sw_min, sw_max)\nax.set(xlabel='Sepal Length', ylabel='Sepal Width', title='KMeans clustering')\nhues = [colormarkers[o][0] for o in y_train]\nmarkers = [colormarkers[o][1] for o in y_train]\nsns.scatterplot(x_train[:,0], x_train[:,1], hue=hues, markers=markers, legend=False)\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f879ee14208&gt;\n\n\n\n\n\n\nSupervised Learning: Regression\n\nfrom sklearn.datasets import load_boston\nfrom sklearn.linear_model import SGDRegressor\nboston = load_boston()\nprint(f'shape - {boston.data.shape}')\nprint(f'boston feature names - {boston.feature_names}')\n\nshape - (506, 13)\nboston feature names - ['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n 'B' 'LSTAT']\n\n\n\nx_train = boston.data\ny_train = boston.target\nx_train = StandardScaler().fit_transform(x_train)\ny_train = StandardScaler().fit_transform(np.expand_dims(y_train, axis=1))\ny_train.shape\n\n(506, 1)\n\n\n\nimport sklearn\ndef train_and_evaluate(model, x_train, y_train, folds):\n    model.fit(x_train, y_train)\n    print(f'score on training set {model.score(x_train, y_train):.2f}')\n    cv = sklearn.cross_validation.KFold(x_train.shape[0], folds, shuffle=True, random_state=33)\n    scores = sklearn.cross_validation.cross_val_score(model, x_train, y_train, cv=cv)\n    print(f'Avg score using {folds} cross validation {np.mean(scores):.2f}')\n\n\nmodel = SGDRegressor(loss='squared_loss', penalty='l2', random_state=33)\ntrain_and_evaluate(model, x_train, y_train, 5)\n\nscore on training set 0.73\nAvg score using 5 cross validation 0.70"
  },
  {
    "objectID": "posts/2023-02-15-codex.html",
    "href": "posts/2023-02-15-codex.html",
    "title": "Results",
    "section": "",
    "text": "Here I am exploring the capabiliteis of Text to SQL of OpenAI codex vs fine-tuned t5-3b model (picard).\nhttps://openai.com/blog/openai-codex/\n\nimport os\nimport openai\nfrom typing import Dict, List\nimport json\nimport requests\n\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\ndef get_schema_string_for_codex(schema_json:Dict):\n  # accepts schema json where key is table name and value is list of columns\n  tables = []\n  for table, columns in schema_json.items():\n    cols = \", \".join(columns)\n    table_string = f\"# Table {table}, columns = [{cols}]\"\n    tables.append(table_string)\n\n  schema_str = \"# SQL\\n\\n\"\n  schema_str += \"\\n\".join(tables)\n  return schema_str\n\n\ndef get_codex_sql(schema, query, print_prompt=True):\n  prompt = \"\\n\\n\".join([schema, query])\n  if print_prompt:\n    print(prompt)\n  response = openai.Completion.create(\n    model=\"code-davinci-002\",\n    prompt=prompt,\n    temperature=0,\n    max_tokens=150,\n    top_p=1.0,\n    frequency_penalty=0.0,\n    presence_penalty=0.0,\n    stop=[\"#\", \";\"]\n  )\n  return response.choices[0].text\n\n\ndef get_schema_string_for_picard(schema_json:Dict, db_name:str):\n  tables = []\n  for table, columns in schema_json.items():\n    cols = \", \".join(columns)\n    table_string = f\"{table} : {cols}\"\n    tables.append(table_string)\n  schema_str = f'| {db_name} | ' + ' | '.join(tables)\n  return schema_str\n\ndef get_picard_sql(schema_str, query, print_prompt=True):\n  payload = json.dumps({\n    \"question\": query,\n    \"db_schema\": schema_str\n  })\n\n  if print_prompt:\n    print(schema_str)\n  headers = {\n    'Content-Type': 'application/json'\n  }\n\n  picard_ip = os.getenv('PICARD_IP')\n  if not picard_ip:\n    raise ValueError('PICARD_IP not set')\n\n  url = f'http://{picard_ip}:8000/ask-with-schema/'\n  response = requests.request(\"POST\", url, headers=headers, data=payload)\n  return response.json()[0]\n\n\n\nschema = {\n    \"albums\": [\"AlbumId\", \"Title\", \"ArtistId\"],\n    \"artists\": [\"ArtistId\", \"Name\"],\n    \"media_types\": [\"MediaTypeId\", \"Name\"],\n    \"playlists\": [\"PlaylistId\", \"Name\"],\n    \"playlist_track\": [\"PlaylistId\", \"TrackId\"],\n    \"tracks\": [\"TrackId\", \"Name\", \"AlbumId\", \"MediaTypeId\", \"GenreId\", \"Composer\", \"Milliseconds\", \"Bytes\", \"UnitPrice\"]\n}\n\nquery = 'generate sql query to list all albums by Adele'\nquery = 'genereate sql to find artists with longest average track length'\nquery = 'generate sql to find the most expensive albums'\nquery = 'generate sql to find duration of tracks from album abc in playlist xyz'\nquery = 'generate sql to find total duration of tracks from album abc in playlist xyz'\n\n\nschema = {\n    \"well\" : [\"id, country, field, latitude, longitude, uwi, well_name\"],\n    \"well_bore\": [\"id, spud_date, ubhi, well_bore_name, well_id\"],\n    \"section\": [\"id, max_dls, max_inclination, min_dls, min_inclination, section_bottom_depth_md, section_caption, section_diameter, section_number, section_top_depth_md, well_bore_id\"],\n    \"operation\": [\"id, operation_code, operation_enddate_time, operation_end_depth, operation_start_datetime, operation_start_depth\"],\n    \"operating_parameters\": ['id', 'avg_rop', 'flow_rate_high', 'flow_rate_low', 'rpm_high', 'rpm_low', 'wob_high', 'wob_low', 'operation_id'],\n    \"run\": [\"id, run_end_depth, run_number, runs_tart_depth, section_id, operation_id\"],\n    \"bit\": [\"id, bit_run_number, bit_type, grading_out, iadc_number, manufacturer, model_number, primary_od, secondary_od, serial_number, run_id\"]\n}\nquery = \"generete sql to find the count of bit types used in each well\"\nquery = \"Give SQL query for Count number of bits for every section of the well bore Matzen 569\"\nquery = \"give sql query to give distance drilled per hour by section number with wellbore name Prottes\"\nquery = \"grading out of bit in each section with wellbore name Matzen\"\nquery = \"get avg_rop by section name with WellboreName Bockfliess\"\nquery = 'find wells with latitude greater than 30'\nquery = 'find wells with well bores with spud date greater than 2010-01-01'\nquery = 'find avg_rop, section diameter in each section with well bore name Prottes'\n\n\nschema_codex = get_schema_string_for_codex(schema)\nprint(get_codex_sql(schema_codex, query))\n\n# SQL\n\n# Table well, columns = [id, country, field, latitude, longitude, uwi, well_name]\n# Table well_bore, columns = [id, spud_date, ubhi, well_bore_name, well_id]\n# Table section, columns = [id, max_dls, max_inclination, mindls, min_inclination, section_bottom_depth_md, section_caption, section_diameter, section_number, section_top_depth_md, well_bore_id]\n# Table operation, columns = [id, operation_code, operation_enddate_time, operation_end_depth, operation_start_datetime, operation_start_depth]\n# Table run, columns = [id, run_end_depth, run_number, runs_tart_depth, section_id, operation_id]\n# Table bit, columns = [id, bit_run_number, bit_type, grading_out, iadc_number, manufacturer, model_number, primary_od, secondary_od, serial_number, run_id]\n\nGive SQL query for Count number of bits for every section of the well bore Matzen 569\n/1-1\n\nSELECT COUNT(*) FROM bit\nINNER JOIN run ON bit.run_id = run.id\nINNER JOIN section ON run.section_id = section.id\nINNER JOIN well_bore ON section.well_bore_id = well_bore.id\nINNER JOIN well ON well_bore.well_id = well.id\nWHERE well.well_name = 'Matzen 569/1-1'\n\nGive SQL query for Count number of bits for every section of the well bore Matzen 569/1-1\n\nSELECT COUNT(*) FROM bit\nINNER JOIN run ON bit\n\n\n\nschema_picard = get_schema_string_for_picard(schema, 'wells')\nprint('\\n' + query + '\\n\\n' + get_picard_sql(schema_picard, query))\n\n| wells | well : id, country, field, latitude, longitude, uwi, well_name | well_bore : id, spud_date, ubhi, well_bore_name, well_id | section : id, max_dls, max_inclination, min_dls, min_inclination, section_bottom_depth_md, section_caption, section_diameter, section_number, section_top_depth_md, well_bore_id | operation : id, operation_code, operation_enddate_time, operation_end_depth, operation_start_datetime, operation_start_depth | operating_parameters : id, avg_rop, flow_rate_high, flow_rate_low, rpm_high, rpm_low, wob_high, wob_low, operation_id | run : id, run_end_depth, run_number, runs_tart_depth, section_id, operation_id | bit : id, bit_run_number, bit_type, grading_out, iadc_number, manufacturer, model_number, primary_od, secondary_od, serial_number, run_id\n\nfind avg_rop, section diameter in each section with well bore name Prottes\n\nselect t1.avg_rop, t1.section_diameter from operating_parameters as t1 join section as t2 on t1.section_id = t2.section_id join well_bore as t3 on t3.id = t2.well_bore_id where t3.well_bore_name = \"Prottes\"\n\n\n\n\n\n\n\n\n\n\n\n\nquery\ncodex_sql\nco_res\npicard_sql\npi_res\n\n\n\n\ngenerate sql query to list all albums by Adele\nSELECT * FROM albums JOIN artists ON artists.ArtistId = albums.ArtistId WHERE artists.Name = ‘Adele’\n\nselect t1.title from albums as t1 join artists as t2 on t1.ArtistId = t2.ArtistId where t2.Name = “Adele”\n\n\n\ngenereate sql to find artists with longest average track length\nSELECT a.Name, AVG(t.Milliseconds) AS AverageTrackLength FROM artists a JOIN albums al ON a.ArtistId = al.ArtistId JOIN tracks t ON al.AlbumId = t.AlbumId GROUP BY a.Name ORDER BY AverageTrackLength DESC LIMIT 1 FROM artists a JOIN albums al ON a.ArtistId = al.ArtistId JOIN tracks t ON al.AlbumId = t.AlbumId GROUP BY a.Name ORDER BY AverageTrackLength DESC LIMIT 1\n\nselect t1.Name from artists as t1 join tracks as t2 on t1.ArtistId = t2.ArtistId group by t2.ArtistId order by avg(milliseconds) desc limit 1\n\n\n\ngenerate sql to find the most expensive albums\nselect albums.title, tracks.unitprice from albums join tracks on albums.albumid = tracks.albumid order by tracks.unitprice desc limit 1\n\nselect title from albums order by unitprice desc limit 1\n\n\n\ngenerate sql to find duration of tracks from album abc in playlist xyz\nSELECT SUM(Milliseconds) FROM tracks WHERE AlbumId = (SELECT AlbumId FROM albums WHERE Title = ‘abc’) AND TrackId IN (SELECT TrackId FROM playlist_track WHERE PlaylistId = (SELECT PlaylistId FROM playlists WHERE Name = ‘xyz’))\n\nselect t1.milliseconds from tracks as t1 join playlist_track as t2 on t1.trackId = t2.TrackId join albums as t3 on t3.albumId = t2.AlbumId join playlists as t4 on t4.playlistId = t1.playlistId where t3.albumId = “ABC” and t4.name = “xyz”\n\n\n\ngenerate sql to find total duration of tracks from album abc in playlist xyz\nselect sum(milliseconds) from tracks where albumid = (select albumid from albums where title = ‘abc’) and trackid in (select trackid from playlist_track where playlistid = (select playlistid from playlists where name = ‘xyz’))\n\nselect sum(t1.milliseconds) from tracks as t1 join playlist_track as t2 on t1.trackId = t2.TrackId join albums as t3 on t3.albumId = t2.AlbumId join playlists as t4 on t4.playlistId = t1.playlistId where t3.albumId = “ABC” and t4.name = “xyz”"
  },
  {
    "objectID": "posts/2022-06-29-linear-algebra-notes.html",
    "href": "posts/2022-06-29-linear-algebra-notes.html",
    "title": "Properties of Eigen Values and Vectors",
    "section": "",
    "text": "import numpy as np\nimport seaborn as sns\n\n\nA symmetric matrix has real eigen values\n\n\n# create a symmetric matrix\ndef get_random_symmetrix_matrix(n=3):\n    a = np.random.randint(-10,10, (3,3))\n    return (a + a.transpose())/2\n\ns = get_random_symmetrix_matrix()\ns\n\narray([[-5. , -2.5, -2. ],\n       [-2.5, -9. ,  4.5],\n       [-2. ,  4.5,  3. ]])\n\n\n\nw, v = np.linalg.eig(s)\nw, v\n\n(array([  5.22047873,  -5.17812095, -11.04235778]),\n array([[-0.2587962 ,  0.91904091, -0.29730177],\n        [ 0.33248317, -0.20422367, -0.92073212],\n        [ 0.90690654,  0.3371298 ,  0.25271333]]))\n\n\n\n# check the magnitude of the first eigen vector. \nprint(np.dot(v[0], v[0]))\n\n# check the dot product. \nprint(np.dot(v[0], v[1]))\n\n# as dimension is one and dot product is close to zero, they are orthonormal\n\n1.0\n-1.520268617663237e-16\n\n\n\nPower method to find the dominant eigan value\nWe initialize a random vector x, usually 1s. We multiply ax and divide the output to make the\n\nerror = 0.001\nn = 3\n\ns = get_random_symmetrix_matrix(n)\nx = np.array([1.0] * n)\n\ndef get_dominant_eig_value(a):\n    eigs = []\n    eig_old = None\n    x = np.array([1.0] * a.shape[1])\n    while True:\n        x = np.matmul(a,x)\n        eig = np.max(np.abs(x))\n        x = x/eig\n        if eig_old and abs(eig - eig_old) &lt; error:\n            break\n        eig_old = eig\n        eigs.append(eig)\n    return (eig, eigs)\n  \ndom_eig, eigs = get_dominant_eig_value(s)\nprint(f'dominant eig is {dom_eig} in {len(eigs)} iterations with margin {error}')\n\nsns.scatterplot(x=range(len(eigs)), y=eigs)\n\ndominant eig is 11.085887005510221 in 6 iterations with margin 0.001\n\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\nnp.abs(\n\nSyntaxError: unexpected EOF while parsing (2266885346.py, line 1)"
  },
  {
    "objectID": "posts/2021-12-23-la-column-spaces.html",
    "href": "posts/2021-12-23-la-column-spaces.html",
    "title": "MIT 18.065 Linear Algebra, Lecture 1",
    "section": "",
    "text": "How to view matrix multiplication?\n\\[\n\\begin{bmatrix}\n2 & 1 & 3 \\\\\n3 & 1 & 4 \\\\\n5 & 7 & 12\n\\end{bmatrix}{\\cdot}\n\\begin{bmatrix}\nx1 \\\\\nx2 \\\\\nx3\n\\end{bmatrix}\n\\]\nInstead of viewing it as a dot product, the output should be seen as a combination of column vectors in A\n\\[\n\\begin{bmatrix}\n2 \\\\ 3 \\\\ 5\n\\end{bmatrix}\nx1\n\\,+\\,\n\\begin{bmatrix}\n1 \\\\ 1 \\\\ 7\n\\end{bmatrix}\nx2\n\\,+\\,\n\\begin{bmatrix}\n3 \\\\ 4 \\\\ 12\n\\end{bmatrix}\nx3\n\\]\n\n\nBasis and Rank\nThe independent columns using which the other columns can be expressed as a combination of is known Basis for column space. The number of independent columns is known as Rank\n\\[\n\\begin{bmatrix}\n2 & 1 & 3 \\\\\n3 & 1 & 4 \\\\\n5 & 7 & 12\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n2 & 1 \\\\\n3 & 1 \\\\\n5 & 7\n\\end{bmatrix}\n\\begin{bmatrix}\n0 & 0 & 1 \\\\\n\\rowcolor{pink} 0 & 1 & 1 \\\\\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "posts/2022-06-26-quantile-plot.html",
    "href": "posts/2022-06-26-quantile-plot.html",
    "title": "notes",
    "section": "",
    "text": "We will try to understand a quantile plot.\nLets us generate n numbers from a uniform distribution. We will then compare those numbers with uniform and normal distribution\n\nimport numpy as np\nimport seaborn as sns\n\n\nn = 9\na = np.sort(np.random.uniform(1, 10, size=n))\na\n\narray([1.14572246, 4.51022338, 5.33175385, 5.82544947, 7.14328162,\n       8.45844914, 8.84241316, 9.01455406, 9.84060727])\n\n\n\nquantiles = [k/(n+1) for k in range(1, n+1)]\nquantiles \n\n[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n\n\nIf we take the quntiles of a, they will be equally spaced. Now lets divide the area under the target distribution with which we are comparing and read the corresponding values of the random variable.\nIf the array values and the corresponding values from the distribtution are proportional (ie, straight we have a straight line in a plot), then the array is close to the distribution\n\nfrom scipy.stats import norm, uniform\n\n\ndist = norm()\ndist_vals = dist.ppf(quantiles)\nprint(dist_vals)\nax = sns.scatterplot(x=a,y=dist_vals)\nax.set_title('compare with normal dist')\n\n[-1.28155157 -0.84162123 -0.52440051 -0.2533471   0.          0.2533471\n  0.52440051  0.84162123  1.28155157]\n\n\nText(0.5, 1.0, 'compare with normal dist')\n\n\n\n\n\n\ndist = uniform()\ndist_vals = dist.ppf(quantiles)\nprint(dist_vals)\nax = sns.scatterplot(x=a,y=dist_vals)\nax.set_title('compare with uniform dist')\n\n[0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9]\n\n\nText(0.5, 1.0, 'compare with uniform dist')"
  },
  {
    "objectID": "posts/2023-01-23-huggingface-course.html",
    "href": "posts/2023-01-23-huggingface-course.html",
    "title": "1. Transformer Models",
    "section": "",
    "text": "from transformers import pipeline\n\n# to start with we will classify a single sentnence\nclassifier = pipeline(\"sentiment-analysis\")\nprint(classifier(\"I watched a good movie yesterday\"))\n\nNo model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\n\n\n[{'label': 'POSITIVE', 'score': 0.9990124702453613}]\n# classify multiple sentences\nclassifier(['that was a good movie', 'He is unwell'])\n\n[{'label': 'POSITIVE', 'score': 0.9998570680618286},\n {'label': 'NEGATIVE', 'score': 0.9989678859710693}]\n# zero shot classification\n# we havent trained the model on the labels we are using\nzero_shot_classifier = pipeline(\"zero-shot-classification\")\nzero_shot_classifier(\"this is a very interesting course on algebra\", candidate_labels=[\"mathematics\", \"physics\", \"chemistry\", \"biology\"])\n\nNo model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\n\n\n{'sequence': 'this is a very interesting course on algebra',\n 'labels': ['mathematics', 'biology', 'physics', 'chemistry'],\n 'scores': [0.9866520166397095,\n  0.004770115949213505,\n  0.004415604285895824,\n  0.004162236116826534]}\n# text generation\ngenerator = pipeline('text-generation')\ngenerator(\"newtons first law states that\")\n\nNo model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n/Users/achinta/miniforge3/envs/ml/lib/python3.9/site-packages/transformers/generation_utils.py:1296: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 50 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n\n\n[{'generated_text': 'newtons first law states that students under the age of 18 \"shall be admitted to any of these colleges and universities which may be established\" with their respective states of residence and where they may obtain college degrees or certificates.\" Section 1-106 sets forth'}]\n# text generation by specifying a model\ngenerator = pipeline('text-generation', model='distilgpt2')\ngenerator('in this course, we will teach you how to', max_length=30, num_return_sequences=3)\n\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\n[{'generated_text': 'in this course, we will teach you how to use your hands to manipulate your feet to push the button in many directions, and try to help you'},\n {'generated_text': 'in this course, we will teach you how to use them in a way for your students. So if you have any questions or concerns, do you'},\n {'generated_text': 'in this course, we will teach you how to be very confident in what you do during training sessions, during training sessions, and in your training sessions'}]\n# Fill Mask\nunmasker = pipeline('fill-mask')\nunmasker(\"sun rises in the &lt;mask&gt;\", top_k=3)\n\nNo model was supplied, defaulted to distilroberta-base and revision ec58a5b (https://huggingface.co/distilroberta-base).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\n\n\n[{'score': 0.296585351228714,\n  'token': 6360,\n  'token_str': ' sky',\n  'sequence': 'sun rises in the sky'},\n {'score': 0.06161285191774368,\n  'token': 12351,\n  'token_str': ' Arctic',\n  'sequence': 'sun rises in the Arctic'},\n {'score': 0.0532655231654644,\n  'token': 3778,\n  'token_str': ' sun',\n  'sequence': 'sun rises in the sun'}]\n# named entity recognnition\nner = pipeline('ner', grouped_entities=True)\nner(\"My name is Kiran and I work at Amazon\")\n\nNo model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\n/Users/achinta/miniforge3/envs/ml/lib/python3.9/site-packages/transformers/pipelines/token_classification.py:135: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n  warnings.warn(\n\n\n[{'entity_group': 'PER',\n  'score': 0.9954788,\n  'word': 'Kiran',\n  'start': 11,\n  'end': 16},\n {'entity_group': 'ORG',\n  'score': 0.99733573,\n  'word': 'Amazon',\n  'start': 31,\n  'end': 37}]\n# question answering\nquestion_answerer = pipeline('question-answering')\nquestion_answerer(question=\"What is the capital of India?\", context=\"India is a country in South Asia. Its capital is New Delhi\")\n\nNo model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\n\n\n{'score': 0.9939645528793335, 'start': 49, 'end': 58, 'answer': 'New Delhi'}\n# summarizer = pipeline('summarization')\n# summarizer('a stitch in time saves nine', min_length=100, max_length=200)\n# !pip install sentencepiece\nfrom transformers import pipeline\ntranslator = pipeline('translation', model='Helsinki-NLP/opus-mt-fr-en')\ntranslator('Bonjour, comment allez-vous?')\n\n/Users/achinta/miniforge3/envs/ml/lib/python3.9/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n  warnings.warn(\"Recommended: pip install sacremoses.\")\n\n\n[{'translation_text': 'Hello, how are you?'}]"
  },
  {
    "objectID": "posts/2023-01-23-huggingface-course.html#encoder-and-decoder-models",
    "href": "posts/2023-01-23-huggingface-course.html#encoder-and-decoder-models",
    "title": "1. Transformer Models",
    "section": "Encoder and Decoder models",
    "text": "Encoder and Decoder models\nIn encoder models, attention layers can access all words in the input sequence. Pretraining involves corruping an input sequence and predicting the original sequence (say masking). They are best suited for tasks that require a full understanding of the entire sequence, such as sentence classification.\nDecoder models use only the decoder of the model. At each stage, the attention layer can access only words positioned before it in the sentence. These models are called auto-regresive models.\nSequence-to-sequence models, use both parts of the Transformer architecture. At each stage, the attention layers of the encoder can access all the words in the initial sentence, whereas the attention layers of the decoder can only access the words positioned before a given word in the input."
  },
  {
    "objectID": "posts/2023-01-23-huggingface-course.html#loading-a-model",
    "href": "posts/2023-01-23-huggingface-course.html#loading-a-model",
    "title": "1. Transformer Models",
    "section": "Loading a model",
    "text": "Loading a model\n\nfrom transformers import AutoModel\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\nmodel = AutoModel.from_pretrained(checkpoint)\n\nSome weights of the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing DistilBertModel: ['classifier.weight', 'pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\n\n\nfrom transformers import AutoTokenizer\nraw_inputs = [\n    \"I've been waiting for a HuggingFace course my whole life.\",\n    \"I hate this so much!\",\n]\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\ninputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\ninputs\n\n{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n          2607,  2026,  2878,  2166,  1012,   102],\n        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n\n\n\n# if task is sequence classification\nfrom transformers import AutoModelForSequenceClassification\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint)\noutputs = model(**inputs)\nprint(outputs.logits.shape)\nprint(f'outputs - {outputs.logits}')\n\n# to output the probabilities, we need pass it thorugh a softmax\nimport torch\nprobs = torch.softmax(outputs.logits, dim=-1)\nprint(f'probabilities - {probs}')\n\nprint(model.config.id2label)\n\ntorch.Size([2, 2])\noutputs - tensor([[-1.5607,  1.6123],\n        [ 4.1692, -3.3464]], grad_fn=&lt;AddmmBackward0&gt;)\nprobabilities - tensor([[4.0195e-02, 9.5981e-01],\n        [9.9946e-01, 5.4418e-04]], grad_fn=&lt;SoftmaxBackward0&gt;)\n{0: 'NEGATIVE', 1: 'POSITIVE'}\n\n\n\nModels\n\nfrom transformers import BertConfig, BertModel\n\nconfig = BertConfig()\nmodel = BertModel(config)\n\nprint(config)\n\nBertConfig {\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.23.1\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\n\n\nSubword tokenization algorithms rely on the principle that frequently used words should not be split into smaller subwords, but rare words should be decomposed into meaningful subwords.\n\n\nTokenizers\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\ntext = \"The medicine is arsenic album, ars albaam, allium sepa\"\ninput_ids = tokenizer(text, return_tensors=\"pt\").input_ids\nprint(input_ids[0])\n\n# create the text from input ids\nprint(tokenizer.decode(input_ids[0]))\n\n# print each token\ntokens = [token for token in tokenizer.convert_ids_to_tokens(input_ids[0])]\nprint(tokens)\n\n\n# print the vocab size\nprint(tokenizer.vocab_size)\n\ntensor([  101,  1109,  5182,  1110,   170, 22972,  1596,  1312,   117,   170,\n         1733,  2393,  2822,  2312,   117,  1155,  3656, 14516,  4163,   102])\n[CLS] The medicine is arsenic album, ars albaam, allium sepa [SEP]\n['[CLS]', 'The', 'medicine', 'is', 'a', '##rsen', '##ic', 'album', ',', 'a', '##rs', 'al', '##ba', '##am', ',', 'all', '##ium', 'se', '##pa', '[SEP]']\n28996\n\n\nWe noticed that for unknown words, the tokenizer will split them into subwords that are not meaningful. For example, the word “huggingface” is split into “hug”, “##ging”, “##face”. This is because the tokenizer was trained on a vocabulary that did not contain the word “huggingface”.\nBut there is a way to simpler way to get tokens.\n\ntokens = tokenizer.tokenize(text)\nprint(tokens)\n\n# convert to ids\ntoken_ids = tokenizer.convert_tokens_to_ids(tokens)\nprint(token_ids)\n\n['The', 'medicine', 'is', 'a', '##rsen', '##ic', 'album', ',', 'a', '##rs', 'al', '##ba', '##am', ',', 'all', '##ium', 'se', '##pa']\n[1109, 5182, 1110, 170, 22972, 1596, 1312, 117, 170, 1733, 2393, 2822, 2312, 117, 1155, 3656, 14516, 4163]\n\n\nAttention masks are tensors with the exact same shape as the input IDs tensor, filled with 0s and 1s: 1s indicate the corresponding tokens should be attended to, and 0s indicate the corresponding tokens should not be attended to (i.e., they should be ignored by the attention layers of the model)."
  },
  {
    "objectID": "posts/2023-01-23-huggingface-course.html#processing-the-data",
    "href": "posts/2023-01-23-huggingface-course.html#processing-the-data",
    "title": "1. Transformer Models",
    "section": "3.2 Processing the data",
    "text": "3.2 Processing the data\n\nimport torch\nfrom transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n\ncheckpoint = 'bert-base-uncased'\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n\nsequences = [\n    'I have been waiting for a Hugging face course my whole life.',\n    'This course is amazing'\n]\nbatch = tokenizer(sequences, padding=True, truncation=True, return_tensors='pt')\nprint(batch.keys())\n\nbatch['labels'] = torch.tensor([1, 1])\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\nSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/Users/achinta/miniforge3/envs/ml/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n\n\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n\n\n\nloss = model(**batch).loss\nloss.backward()\noptimizer.step()\n\n\n# download and cache datasets\n# mrpc dataset is a dataset for paraphrase detection. \nfrom datasets import load_dataset\nraw_datasets = load_dataset('glue','mrpc')\nraw_datasets\n\nFound cached dataset glue (/Users/achinta/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n\n\n\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx'],\n        num_rows: 3668\n    })\n    validation: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx'],\n        num_rows: 408\n    })\n    test: Dataset({\n        features: ['sentence1', 'sentence2', 'label', 'idx'],\n        num_rows: 1725\n    })\n})\n\n\n\nraw_train_dataset = raw_datasets['train']\nraw_train_dataset[0]\n\n{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n 'label': 1,\n 'idx': 0}\n\n\n\nraw_train_dataset.features\n\n{'sentence1': Value(dtype='string', id=None),\n 'sentence2': Value(dtype='string', id=None),\n 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None),\n 'idx': Value(dtype='int32', id=None)}\n\n\n\nfrom transformers import AutoTokenizer\ncheckpoint = 'bert-base-uncased'\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\ntokenized_sentences_1 = tokenizer(raw_datasets['train']['sentence1'])\nprint(tokenized_sentences_1['input_ids'][0])\n\n# we need sentence pairs, and the tokenizer understands that\ninputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\")\nprint(inputs.keys())\n\nprint(tokenizer.convert_ids_to_tokens(inputs['input_ids']))\n# Here token_type_ids is used to separate the two sentences\n\n[101, 2572, 3217, 5831, 5496, 2010, 2567, 1010, 3183, 2002, 2170, 1000, 1996, 7409, 1000, 1010, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102]\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']\n\n\n\n# We can tokenize the entire dataset in one go using\ntokenized_dataset = tokenizer(raw_datasets['train']['sentence1'], raw_datasets['train']['sentence2'], padding=True, truncation=True)\n\n# to tokenize in batches, we define a function to be applied to each sample. We can also handle any other preprocessing we want to do\ndef tokenize_function(examples):\n    return tokenizer(examples['sentence1'], examples['sentence2'], padding=True, truncation=True)\n\n\ntokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\nprint(f\"raw train dataset has keys - {raw_datasets['train'][0].keys()}\")\nprint(f\"tokenized train dataset has keys - {tokenized_datasets['train'][0].keys()}\")\n\nLoading cached processed dataset at /Users/achinta/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-7570128720f579c6.arrow\nLoading cached processed dataset at /Users/achinta/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-7b7428962528a4a4.arrow\nLoading cached processed dataset at /Users/achinta/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-3dcdb0892050254a.arrow\n\n\nraw train dataset has keys - dict_keys(['sentence1', 'sentence2', 'label', 'idx'])\ntokenized train dataset has keys - dict_keys(['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'])\n\n\n\n# lets find the size of input_ids for a few samples\nimport random\nsamples = random.sample(list(tokenized_datasets[\"train\"]), k=10)\nsamples = [{k: v for k, v in sample.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]} for sample in samples]\n[len(sample['input_ids']) for sample in samples]\n\n[96, 100, 96, 89, 89, 89, 96, 89, 103, 103]\n\n\n\n# lets use the data collator to pad the dataset to the max length in the dataset (not the max length of the dataset)\nfrom transformers import DataCollatorWithPadding\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)    \nbatch = data_collator(samples)\nprint(batch.keys())\n[len(batch['input_ids'][i]) for i in range(len(batch['input_ids']))]\n{k: v.shape for k, v in batch.items()}\n\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n\n\n{'input_ids': torch.Size([10, 103]),\n 'token_type_ids': torch.Size([10, 103]),\n 'attention_mask': torch.Size([10, 103]),\n 'labels': torch.Size([10])}\n\n\n\n3.3 Finetuning the model\nLets sumamrize the steps we did till now\n\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification, Trainer\nraw_datasets = load_dataset('glue','mrpc')\ncheckpoint = 'bert-base-uncased'\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\ndef tokenize_function(examples):\n    return tokenizer(examples['sentence1'], examples['sentence2'], padding=True, truncation=True)\n\ntokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\nfrom transformers import TrainingArguments\ntraining_args = TrainingArguments('test-trainer')\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n\nFound cached dataset glue (/Users/achinta/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\nloading configuration file config.json from cache at /Users/achinta/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\nModel config BertConfig {\n  \"_name_or_path\": \"bert-base-uncased\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.23.1\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nloading file vocab.txt from cache at /Users/achinta/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/vocab.txt\nloading file tokenizer.json from cache at /Users/achinta/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/tokenizer.json\nloading file added_tokens.json from cache at None\nloading file special_tokens_map.json from cache at None\nloading file tokenizer_config.json from cache at /Users/achinta/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/tokenizer_config.json\nloading configuration file config.json from cache at /Users/achinta/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\nModel config BertConfig {\n  \"_name_or_path\": \"bert-base-uncased\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.23.1\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nLoading cached processed dataset at /Users/achinta/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-df486dc2eb69ac71.arrow\nLoading cached processed dataset at /Users/achinta/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-782aec776148990b.arrow\nLoading cached processed dataset at /Users/achinta/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-8daf6f80b8639cc1.arrow\nPyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\nloading configuration file config.json from cache at /Users/achinta/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\nModel config BertConfig {\n  \"_name_or_path\": \"bert-base-uncased\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.23.1\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /Users/achinta/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/pytorch_model.bin\nSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\n\n\n\nimport evaluate\nimport numpy as np\n\ndef compute_metrics(eval_preds):\n    metric = evaluate.load(\"glue\", \"mrpc\")\n    logits, labels = eval_preds\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)\n\ntrainer = Trainer(\n    model,\n    training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()\n\nwandb: Network error (ConnectTimeout), entering retry loop.\n\n\nWe get a warning after instantiating this pretrained model. This is because BERT has not been pretrained on classifying pairs of sentences, so the head of the pretrained model has been discarded and a new head suitable for sequence classification has been added instead. The warnings indicate that some weights were not used (the ones corresponding to the dropped pretraining head) and that some others were randomly initialized (the ones for the new head)."
  },
  {
    "objectID": "posts/2023-01-23-huggingface-course.html#the-datasets-library",
    "href": "posts/2023-01-23-huggingface-course.html#the-datasets-library",
    "title": "1. Transformer Models",
    "section": "5 The datasets library",
    "text": "5 The datasets library\n\n!export TOKENIZERS_PARALLELISM=true\nfrom datasets import load_dataset\n# !wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz\n# !wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz\n\nsquad_it_dataset = load_dataset('json', data_files='SQuAD_it-train.json.gz', field='data')\nsquad_it_dataset\n\n# we can include train and tests data in the squad_it_dataset\ndata_files = {\"train\": \"SQuAD_it-train.json.gz\", \"test\": \"SQuAD_it-test.json.gz\"}\nsquad_it_dataset = load_dataset('json',data_files=data_files,field='data' )\nsquad_it_dataset\n\n# we give remote urls also\n\nUsing custom data configuration default-bbdcaac21d7e3d0b\nFound cached dataset json (/Users/achinta/.cache/huggingface/datasets/json/default-bbdcaac21d7e3d0b/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\nUsing custom data configuration default-80fa3afbe58e2f42\nFound cached dataset json (/Users/achinta/.cache/huggingface/datasets/json/default-80fa3afbe58e2f42/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n\n\n\n\n\n\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['title', 'paragraphs'],\n        num_rows: 442\n    })\n    test: Dataset({\n        features: ['title', 'paragraphs'],\n        num_rows: 48\n    })\n})\n\n\n\n%%time\n# !wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\"\n# !unzip drugsCom_raw.zip\nfrom datasets import load_dataset\n!ls -lrth drugs*.tsv\n\ndata_files = {\"train\": \"drugsComTrain_raw.tsv\", \"test\": \"drugsComTest_raw.tsv\"}\ndrug_dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\")\ndrug_dataset\n\n-rw-r--r--  1 achinta  staff    80M Oct  2  2018 drugsComTrain_raw.tsv\n-rw-r--r--  1 achinta  staff    27M Oct  2  2018 drugsComTest_raw.tsv\nCPU times: user 69.1 ms, sys: 17.9 ms, total: 87 ms\nWall time: 1.52 s\n\n\nUsing custom data configuration default-4eaca5caac99961c\nFound cached dataset csv (/Users/achinta/.cache/huggingface/datasets/csv/default-4eaca5caac99961c/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n\n\n\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['Unnamed: 0', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n        num_rows: 161297\n    })\n    test: Dataset({\n        features: ['Unnamed: 0', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n        num_rows: 53766\n    })\n})\n\n\n\n# look at a sample of the data\ndrug_sample = drug_dataset[\"train\"].shuffle(seed=42).select(range(1000))\ndrug_sample[:3]\n\nLoading cached shuffled indices for dataset at /Users/achinta/.cache/huggingface/datasets/csv/default-4eaca5caac99961c/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a77308af6ca3c149.arrow\n\n\n{'Unnamed: 0': [87571, 178045, 80482],\n 'drugName': ['Naproxen', 'Duloxetine', 'Mobic'],\n 'condition': ['Gout, Acute', 'ibromyalgia', 'Inflammatory Conditions'],\n 'review': ['\"like the previous person mention, I&#039;m a strong believer of aleve, it works faster for my gout than the prescription meds I take. No more going to the doctor for refills.....Aleve works!\"',\n  '\"I have taken Cymbalta for about a year and a half for fibromyalgia pain. It is great\\r\\nas a pain reducer and an anti-depressant, however, the side effects outweighed \\r\\nany benefit I got from it. I had trouble with restlessness, being tired constantly,\\r\\ndizziness, dry mouth, numbness and tingling in my feet, and horrible sweating. I am\\r\\nbeing weaned off of it now. Went from 60 mg to 30mg and now to 15 mg. I will be\\r\\noff completely in about a week. The fibro pain is coming back, but I would rather deal with it than the side effects.\"',\n  '\"I have been taking Mobic for over a year with no side effects other than an elevated blood pressure.  I had severe knee and ankle pain which completely went away after taking Mobic.  I attempted to stop the medication however pain returned after a few days.\"'],\n 'rating': [9.0, 3.0, 10.0],\n 'date': ['September 2, 2015', 'November 7, 2011', 'June 5, 2013'],\n 'usefulCount': [36, 13, 128]}\n\n\n\n# check that the \"Unnamed: 0\" is unique key in the dataset\nfor split in drug_dataset.keys():\n    assert len(drug_dataset[split]) == len(drug_dataset[split].unique(\"Unnamed: 0\"))\n\n# lets rename the column\ndrug_dataset = drug_dataset.rename_column(\n    original_column_name=\"Unnamed: 0\", new_column_name=\"patient_id\"\n)\ndrug_dataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n        num_rows: 161297\n    })\n    test: Dataset({\n        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n        num_rows: 53766\n    })\n})\n\n\n\ndef lowercase_condition(example):\n    return {'condition': example['condition'].lower()}\n\ndef filter_nones(x):\n    return x['condition'] is not None\n\ndrug_dataset = drug_dataset.filter(lambda x: x[\"condition\"] is not None)\n\ndrug_dataset = drug_dataset.map(lowercase_condition)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef compute_review_length(example):\n    return {\"review_length\": len(example[\"review\"].split())}\n\ndrug_dataset = drug_dataset.map(compute_review_length)\n\n\n\n\n\n\n\n\ndrug_dataset[\"train\"].sort(\"review_length\")[:2]\n\nLoading cached sorted indices for dataset at /Users/achinta/.cache/huggingface/datasets/csv/default-4eaca5caac99961c/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-75752611952acc02.arrow\n\n\n{'patient_id': [103488, 23627],\n 'drugName': ['Loestrin 21 1 / 20', 'Chlorzoxazone'],\n 'condition': ['birth control', 'muscle spasm'],\n 'review': ['\"Excellent.\"', '\"useless\"'],\n 'rating': [10.0, 1.0],\n 'date': ['November 4, 2008', 'March 24, 2017'],\n 'usefulCount': [5, 2],\n 'review_length': [1, 1]}\n\n\n\nimport html\n\ntext = \"I&#039;m a transformer called BERT\"\nhtml.unescape(text)\n\ndrug_dataset = drug_dataset.map(lambda x: {\"review\": html.unescape(x[\"review\"])})\n\n\n\n\n\n\n\n\nnew_drug_dataset = drug_dataset.map(\n    lambda x: {\"review\": [html.unescape(o) for o in x[\"review\"]]}, batched=True\n)"
  },
  {
    "objectID": "posts/2023-02-23-spider-dataset.html",
    "href": "posts/2023-02-23-spider-dataset.html",
    "title": "notes",
    "section": "",
    "text": "We will covert the spider dataset to a networkx graph.\n\n# !pip install gdown\n!gdown \"https://drive.google.com/uc?export=download&id=1_AckYkinAnhqmRQtGsQgUKAnTHxxX5J0\"\n\nDownloading...\nFrom: https://drive.google.com/uc?export=download&id=1_AckYkinAnhqmRQtGsQgUKAnTHxxX5J0\nTo: /Users/achinta/github/notes_old/_notebooks/spider.zip\n100%|██████████████████████████████████████| 99.7M/99.7M [00:13&lt;00:00, 7.45MB/s]\n\n\n\nimport networkx as nx\nG = nx.Graph()"
  },
  {
    "objectID": "posts/2021-11-22-01-the-machine-learning-landscape.html",
    "href": "posts/2021-11-22-01-the-machine-learning-landscape.html",
    "title": "Hands - on ML. Chapter 1",
    "section": "",
    "text": "import sys, os\nimport sklearn\nimport urllib.request\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport random\n\n# matplotlib\nimport matplotlib\nmatplotlib.rc('axes', labelsize=14),\nmatplotlib.rc('xtick', labelsize=12)\nmatplotlib.rc('ytick', labelsize=12)\nprint(f'sklearn_version - {sklearn.__version__}')\nimport seaborn as sns\n\nsklearn_version - 0.24.2\n\n\n\n# just copying it from github. Prepreocessing code\ndef prepare_country_stats(oecd_bli, gdp_per_capita):\n    oecd_bli = oecd_bli[oecd_bli[\"INEQUALITY\"]==\"TOT\"]\n    oecd_bli = oecd_bli.pivot(index=\"Country\", columns=\"Indicator\", values=\"Value\")\n    gdp_per_capita.rename(columns={\"2015\": \"GDP per capita\"}, inplace=True)\n    gdp_per_capita.set_index(\"Country\", inplace=True)\n    full_country_stats = pd.merge(left=oecd_bli, right=gdp_per_capita,\n                                  left_index=True, right_index=True)\n    full_country_stats.sort_values(by=\"GDP per capita\", inplace=True)\n    remove_indices = [0, 1, 6, 8, 33, 34, 35]\n    keep_indices = list(set(range(36)) - set(remove_indices))\n    return full_country_stats[[\"GDP per capita\", 'Life satisfaction']].iloc[keep_indices]\n\n\n!curl https://raw.githubusercontent.com/ageron/handson-ml2/master/oecd_bli_2015.csv\n\n404: Not Found\n\n\n\n# download the data\nDOWNLOAD_ROOT =  \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\ndatapath = Path('datasets/lifesat')\ndatapath.mkdir(exist_ok=True, parents=True )\n\nfor filename in (\"oecd_bli_2015.csv\", \"gdp_per_capita.csv\"):\n    url = '/'.join([DOWNLOAD_ROOT, 'datasets/lifesat',filename])\n    filepath = (datapath/filename).resolve()\n    print(f'downloading from {url}')\n    urllib.request.urlretrieve(url, filepath)\n\ndownloading from https://raw.githubusercontent.com/ageron/handson-ml2/master//datasets/lifesat/oecd_bli_2015.csv to /home/achinta/machine-learning/books/handson-ml2/datasets/lifesat/oecd_bli_2015.csv\ndownloading from https://raw.githubusercontent.com/ageron/handson-ml2/master//datasets/lifesat/gdp_per_capita.csv to /home/achinta/machine-learning/books/handson-ml2/datasets/lifesat/gdp_per_capita.csv\n\n\n\nPlot the data\n\noecd_bli = pd.read_csv(datapath/'oecd_bli_2015.csv', thousands=',')\ngdp_per_capita = pd.read_csv(datapath/'gdp_per_capita.csv', thousands=',', delimiter='\\t', encoding='latin1', na_values='n/a')\ncountry_stats = prepare_country_stats(oecd_bli, gdp_per_capita)\nprint(f'country stats.shape - {country_stats.shape}')\n\n# show the data in a plot\nfig, ax = plt.subplots(figsize=(8,4))\nsns.scatterplot(x=country_stats['GDP per capita'], y=country_stats['Life satisfaction'], ax=ax)\n\n# lets annotate a few examples\nidxs = random.sample(range(country_stats.shape[0]), k=5)\nfor idx in idxs:\n    row = country_stats.iloc[idx]\n    plt.annotate(row.name, xy=(row['GDP per capita'], row['Life satisfaction']), \n                 arrowprops=dict(facecolor='black', width=0.5, shrink=0.1, headwidth=5),\n                xytext=(row['GDP per capita'],row['Life satisfaction'] - 0.5 ))\n\nplt.show()\n\ncountry stats.shape - (29, 2)\n\n\n\n\n\n\n\nmodel\n\nimport sklearn.linear_model\n\nx=country_stats['GDP per capita'].values\ny=country_stats['Life satisfaction'].values\n                                                   \nmodel = sklearn.linear_model.LinearRegression()\nmodel.fit(x[:, np.newaxis], y)\n\nLinearRegression()\n\n\n\n# Make a prediction for Cyprus\nX_new = [[22587]]  # Cyprus' GDP per capita\nprint(model.predict(X_new)) # outputs [[ 5.96242338]]\n\n[5.96242338]"
  },
  {
    "objectID": "posts/2021-11-11-linear-algebra-svd.html",
    "href": "posts/2021-11-11-linear-algebra-svd.html",
    "title": "Fastai Linear Algebra - chapter 1",
    "section": "",
    "text": "# !pip install imageio-ffmpeg moviepy imageio\n\nimport imageio\nimport moviepy.editor as mpe\nimport numpy as np\nimport scipy\nfrom pathlib import Path\nfrom PIL import Image\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\n \nnp.set_printoptions(precision=4, suppress=True)\n\n\n# download video from https://github.com/momonala/image-processing-projects/blob/master/background_removal/video/Video_003/Video_003.avi\n!curl -o Video_003.avi https://raw.githubusercontent.com/momonala/image-processing-projects/master/background_removal/video/Video_003/Video_003.avi\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 1241k  100 1241k    0     0  3505k      0 --:--:-- --:--:-- --:--:-- 3495k\n\n\n\nvideo = mpe.VideoFileClip(\"Video_003.avi\")\nprint(f'fps - {video.fps} and duration is {video.duration}')\n\nfps - 7.0 and duration is 113.57\n\n\n\nclip = video.subclip(0, 50).ipython_display(width=300)\nclip\n\nMoviepy - Building video __temp__.mp4.\nMoviepy - Writing video __temp__.mp4\n\nMoviepy - Done !\nMoviepy - video ready __temp__.mp4\n\n\n                                                                \n\n\nSorry, seems like your browser doesn't support HTML5 audio/video\n\n\n\nHelper Methods\n\ndef resize(img_array, dims):\n    return np.array(Image.fromarray(img_array).resize(size=dims))\n\n\n\ndef rgb2gray(rgb):\n    # return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\n\n\n\nFormat the Data\nAn image from 1 moment in time is 120 pixels by 160 pixels (when scaled). We can unroll that picture into a single tall column. So instead of having a 2D picture that is 120×160, we have a 1×19,200 column\nThis isn’t very human-readable, but it’s handy because it lets us stack the images from different times on top of one another, to put a video all into 1 matrix. If we took the video image every hundredth of a second for 100 seconds (so 10,000 different images, each from a different point in time), we’d have a 10,000×19,200 matrix, representing the video!\n\nscale = 0.5 # adjust scale to change resolution of image\ndims = (int(240 * scale), int(320 * scale))\nprint(f'dims - {dims}')\nfps = video.fps\nfps\n\ndims - (120, 160)\n\n\n7.0\n\n\n\nclip = video.subclip(0, 1000)\nframe = clip.get_frame(100/fps)\n# frame[..., :3]\ngray = rgb2gray(frame)\ngray.shape\nresized = resize(gray, dims)\nprint(resized.shape)\n\n(160, 120)\n\n\n\n%%time\ndef create_data_matrix_from_video(clip, fps=5, scale=0.5):\n    # get dimension of each frame\n    dims = clip.get_frame(0).shape[:2]\n    \n    # get scaled dimensions\n    dims = [int(o*scale) for o in dims]\n    print(dims)\n    return np.vstack([resize(rgb2gray(clip.get_frame(i/float(fps))), dims).astype(int).flatten() for i in range(int(fps) * int(clip.duration))]).T\n\nM = create_data_matrix_from_video(video.subclip(0,100), fps, scale)\nprint(M.shape, M.dtype)\n\n[120, 160]\n(19200, 700) int64\nCPU times: user 2.21 s, sys: 163 ms, total: 2.38 s\nWall time: 2.86 s\n\n\n\n\nSVD\nSVD for a matrix A is given by\n\\[\nA_{(m,n)} = U_{(m,m)}{\\cdot}Sigma_{(m,n)}{\\cdot}V^T_{(n,n)}\n\\]\n\nU is called left singular matrix\nSigma is called sing\n\nSigma is a diagonal matrix. Lets check with an example\n\nfrom scipy.linalg import svd\nfrom typing import Tuple\n\ndef compute_svd(shape: Tuple, full_matrices=True):\n    A = np.random.randn(*shape)\n    U, s, VT = svd(A, full_matrices=full_matrices)\n    print(f'{A.shape} Matrix with full_matrices {full_matrices}, decomposes into', U.shape, s.shape, VT.shape)\n    return U, s, VT\n\ndef reconstruct(U, s, VT):\n    m = U.shape[0]\n    n = VT.shape[0]\n    \n    sigma = np.zeros((m,n))\n    if m &gt; n:\n        sigma[:n, :] = np.diag(s) \n    else: \n        sigma[:, :m] = np.diag(s)\n    return np.linalg.multi_dot([U, sigma, VT])\n        \n    \n\n_, _, _ = compute_svd((2,3), full_matrices=True)\n_, _, _ = compute_svd((3,2), full_matrices=True)\n\n\n_, _, _ = compute_svd((3,10), full_matrices=True)\n_, _, _ = compute_svd((3,10), full_matrices=False)\n\n(2, 3) Matrix with full_matrices True, decomposes into (2, 2) (2,) (3, 3)\n(3, 2) Matrix with full_matrices True, decomposes into (3, 3) (2,) (2, 2)\n(3, 10) Matrix with full_matrices True, decomposes into (3, 3) (3,) (10, 10)\n(3, 10) Matrix with full_matrices False, decomposes into (3, 3) (3,) (3, 10)\n\n\n\nCheck that we are able to reconstruct the matrix\n\nA = np.random.randn(4, 3)\nU, s, VT = svd(A, full_matrices=full_matrices)\nA_recons = reconstruct(U, s, VT)\nnp.allclose(A, A_recons)\n\nTrue\n\n\n\n\nPseudoinverse\n\nMatrix inversion is not defined for matrices that are not square. […] When A has more columns than rows, then solving a linear equation using the pseudoinverse provides one of the many possible solutions.\n\n\n%%time\nU, s, V = np.linalg.svd(M, full_matrices=False)\nprint(M.shape, U.shape, s.shape, V.shape)\n\n(3, 2) (3, 2) (2,) (2, 2)\nCPU times: user 358 µs, sys: 468 µs, total: 826 µs\nWall time: 810 µs\n\n\n\n%%time\nfrom scipy.linalg import svd\nU, s, VT = svd(M)\nprint(M.shape, U.shape, s.shape, VT.shape)\n\n(19200, 700) (19200, 19200) (700,) (700, 700)\nCPU times: user 2min 6s, sys: 2.86 s, total: 2min 8s\nWall time: 1min 11s\n\n\n\nM = np.random.randn(3,2)\nU, s, VT = svd(M)\nprint(M.shape, U.shape, s.shape, VT.shape)\n\n(3, 2) (3, 3) (2,) (2, 2)"
  },
  {
    "objectID": "posts/2021-12-28-variational-inference.html",
    "href": "posts/2021-12-28-variational-inference.html",
    "title": "Variational Inference",
    "section": "",
    "text": "Following the blog post https://willcrichton.net/notes/probabilistic-programming-under-the-hood/\n\nimport pyro.distributions as dist\nfrom pyro import sample\nimport torch\n\n\n# A fair coin flip\ncoinflip = sample(\"coinflip\", dist.Bernoulli(probs=0.5))\nprint(f'coinflip - {coinflip}')\n\n# Noisy sample\nnoisy_sample = sample(\"noisy_sample\", dist.Normal(loc=0, scale=1))\nprint(f'Noisy Sample - {noisy_sample}')\n\ncoinflip - 1.0\nNoisy Sample - -0.21278521418571472\n\n\n\ndef sleep_model():\n    # very likely to feel lazy\n    feeling_lazy = sample('feeling_lazy', dist.Bernoulli(0.9))\n    if feeling_lazy:\n        # only going to possibly ignore alarm if I am feeling lazy\n        ignore_alarm = sample('ignore_alarm', dist.Bernoulli(0.8))\n        # will sleep more if Ignore alarm\n        amount_slept = sample(f'amount_slept', dist.Normal(8 + 2*ignore_alarm, 1))\n    else:\n        amount_slept = sample('amount_slept', dist.Normal(6, 1))\n    return amount_slept\n\nprint(sleep_model())\nprint(sleep_model())\nprint(sleep_model())\n                              \n\ntensor(9.6567)\ntensor(8.2041)\ntensor(11.7966)"
  },
  {
    "objectID": "posts/2021-12-28-variational-inference.html#approximate-inference",
    "href": "posts/2021-12-28-variational-inference.html#approximate-inference",
    "title": "Variational Inference",
    "section": "Approximate Inference",
    "text": "Approximate Inference\nThe main idea is that instead of exactly computing the conditional probability distribution (or “posterior”) of interest, we can approximate it using a variety of techniques. Generally, these fall into two camps: sampling methods and variational methods. The CS 228 (Probabilistic Graphical Models at Stanford) course notes go in depth on both (sampling, Variational)\nEssentially, for sampling methods, you use algorithms that continually draw samples from a changing probability distribution until eventually they converge on the true posterior of interest. The time to convergence is not known ahead of time. For variational methods, you use a simpler function that can be optimized to match the true posterior using standard optimization techniques like gradient descent.\nWhere to use what? Please have a look at the original blog post.\n\nVariational inference 1: autodifferentiation\n\nnorm = dist.Normal(0, 1)\nx = sample('x', norm)\nx\n\ntensor(-0.8995)\n\n\nHowever, let’s say I know the value of x = 5 and I want to find a mean μ to the normal distribution that maximizes the probability of seeing that x. For that, we can use a parameter:\n\nfrom pyro import param\n\nmu = param(\"mu\", torch.tensor(0.0))\nnorm = dist.Normal(mu, 1)\nx = sample('x', norm)\n\nOur goal is to update mu such that the probability of the value 5 under the normal distribution norm is maximized.\n\nimport torch.nn as nn\nimport torch.distributions as dist\nfrom torch.optim import Adam\n\nclass NormalDistModel(nn.Module):\n    def __init__(self, mu):\n        super().__init__()\n        self.mu = nn.Parameter(torch.tensor(mu, dtype=torch.float32))\n        self.normal = dist.Normal(self.mu, 1)\n        \n    def forward(self):\n        return self.normal.log_prob(torch.tensor(5.0))\n        \n\nmodel = NormalDistModel(1)\nmodel.forward()\n\ntensor(-8.9189, grad_fn=&lt;SubBackward0&gt;)\n\n\n\n# lets now optimize the model to maximize the probabiliy of 5\noptimizer = Adam(model.parameters(), lr=0.01)\n\nmus = []\nlosses = []\nfor _ in range (1000):\n    # instead of maximizing the probablity, we minize the negative log of the probability\n    loss = -model.forward()\n    loss.backward()\n    \n    # update parameters\n    optimizer.step()\n    \n    # zero all parameter gradients so that they do not accumulate\n    optimizer.zero_grad()\n    \n    # record the mu and the and the loss\n    losses.append(loss.detach().item())\n    mus.append(model.mu.detach().item())\n    \n\n\ndf = pd.DataFrame({\"mu\": mus, \"loss\": losses})\ndf.plot(subplots=True)\n\narray([&lt;AxesSubplot:&gt;, &lt;AxesSubplot:&gt;], dtype=object)"
  },
  {
    "objectID": "posts/2022-03-07-product-recomendation.html",
    "href": "posts/2022-03-07-product-recomendation.html",
    "title": "Satender Product Recommendation",
    "section": "",
    "text": "# Download the data\n#!kaggle competitions download -c santander-product-recommendation\n#!mv santander-product-recommendation.zip data/\n# !cd data;unzip santander-product-recommendation.zip;unzip train_ver2.csv.zip;unzip test_ver2.csv.zip\n!ls -lrth data\n\ntotal 2.7G\n-rw-r--r-- 1 achinta achinta 106M Oct 27  2016 test_ver2.csv\n-rwxrwxrwx 1 achinta achinta 2.2G Oct 27  2016 train_ver2.csv\n-rw-rw-r-- 1 achinta achinta  13M Dec 11  2019 test_ver2.csv.zip\n-rw-rw-r-- 1 achinta achinta 2.3M Dec 11  2019 sample_submission.csv.zip\n-rw-rw-r-- 1 achinta achinta 215M Dec 11  2019 train_ver2.csv.zip\n-rw-rw-r-- 1 achinta achinta 229M Mar  7 10:30 santander-product-recommendation.zip\n\n\n\n! wc -l data/train_ver2.csv\n\n13647310 data/train_ver2.csv\n\n\nSo we have 13.6M rows in the training dataset\n\nSVD\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.linalg import sqrtm\nfrom copy import deepcopy\n\n\ntrain_10k = pd.read_csv('data/train_ver2.csv', nrows=10000)\nprint(f'columns- {train_10k.columns}')\n\ncolumns- Index(['fecha_dato', 'ncodpers', 'ind_empleado', 'pais_residencia', 'sexo',\n       'age', 'fecha_alta', 'ind_nuevo', 'antiguedad', 'indrel',\n       'ult_fec_cli_1t', 'indrel_1mes', 'tiprel_1mes', 'indresi', 'indext',\n       'conyuemp', 'canal_entrada', 'indfall', 'tipodom', 'cod_prov',\n       'nomprov', 'ind_actividad_cliente', 'renta', 'segmento',\n       'ind_ahor_fin_ult1', 'ind_aval_fin_ult1', 'ind_cco_fin_ult1',\n       'ind_cder_fin_ult1', 'ind_cno_fin_ult1', 'ind_ctju_fin_ult1',\n       'ind_ctma_fin_ult1', 'ind_ctop_fin_ult1', 'ind_ctpp_fin_ult1',\n       'ind_deco_fin_ult1', 'ind_deme_fin_ult1', 'ind_dela_fin_ult1',\n       'ind_ecue_fin_ult1', 'ind_fond_fin_ult1', 'ind_hip_fin_ult1',\n       'ind_plan_fin_ult1', 'ind_pres_fin_ult1', 'ind_reca_fin_ult1',\n       'ind_tjcr_fin_ult1', 'ind_valo_fin_ult1', 'ind_viv_fin_ult1',\n       'ind_nomina_ult1', 'ind_nom_pens_ult1', 'ind_recibo_ult1'],\n      dtype='object')\n\n\nHere are the columns we will be using in the SVD approach. We are ignoring the categorical features. (I am not sure if they can be used in SVD)\n\n\n\nColumn\nDescription\n\n\n\n\nfetcha_dato\nMonth of Purchase\n\n\nnocdepers\nCustmomer Code\n\n\nind__(xyz)__ult1\nProducts on which we have to predict sales upon\n\n\n\n\n\n\n\n\n\n\n\n\n%%time\ntrain_dates = pd.read_csv(f'data/train_ver2.csv', usecols=['fecha_dato'])\ntest_dates = pd.read_csv(f'data/test_ver2.csv', usecols=['fecha_dato'])\nprint(f'train data has {train_dates.shape[0]/1e6}M rows')\nprint(f'test  data has {test_dates.shape[0]/1e3}K rows')\n\nprint(f'train months {sorted(train_dates.fecha_dato.unique())}')\nprint(f'test months {sorted(test_dates.fecha_dato.unique())}')\n\ntrain data has 13.647309M rows\ntest  data has 929.615K rows\ntrain months ['2015-01-28', '2015-02-28', '2015-03-28', '2015-04-28', '2015-05-28', '2015-06-28', '2015-07-28', '2015-08-28', '2015-09-28', '2015-10-28', '2015-11-28', '2015-12-28', '2016-01-28', '2016-02-28', '2016-03-28', '2016-04-28', '2016-05-28']\ntest months ['2016-06-28']\nCPU times: user 11.6 s, sys: 676 ms, total: 12.3 s\nWall time: 12.3 s\n\n\n\ntrain_10k.head(2)\n\n\n\n\n\n\n\n\nfecha_dato\nncodpers\nind_empleado\npais_residencia\nsexo\nage\nfecha_alta\nind_nuevo\nantiguedad\nindrel\n...\nind_hip_fin_ult1\nind_plan_fin_ult1\nind_pres_fin_ult1\nind_reca_fin_ult1\nind_tjcr_fin_ult1\nind_valo_fin_ult1\nind_viv_fin_ult1\nind_nomina_ult1\nind_nom_pens_ult1\nind_recibo_ult1\n\n\n\n\n0\n2015-01-28\n1375586\nN\nES\nH\n35\n2015-01-12\n0.0\n6\n1.0\n...\n0\n0\n0\n0\n0\n0\n0\n0.0\n0.0\n0\n\n\n1\n2015-01-28\n1050611\nN\nES\nV\n23\n2012-08-10\n0.0\n35\n1.0\n...\n0\n0\n0\n0\n0\n0\n0\n0.0\n0.0\n0\n\n\n\n\n2 rows × 48 columns\n\n\n\n\nsvd_cols = ['fecha_dato', 'ncodpers', 'ind_ahor_fin_ult1', 'ind_aval_fin_ult1', 'ind_cco_fin_ult1','ind_cder_fin_ult1', 'ind_cno_fin_ult1', 'ind_ctju_fin_ult1',\n       'ind_ctma_fin_ult1', 'ind_ctop_fin_ult1', 'ind_ctpp_fin_ult1','ind_deco_fin_ult1', 'ind_deme_fin_ult1', 'ind_dela_fin_ult1','ind_ecue_fin_ult1', 'ind_fond_fin_ult1', 'ind_hip_fin_ult1',\n       'ind_plan_fin_ult1', 'ind_pres_fin_ult1', 'ind_reca_fin_ult1','ind_tjcr_fin_ult1', 'ind_valo_fin_ult1', 'ind_viv_fin_ult1','ind_nomina_ult1', 'ind_nom_pens_ult1', 'ind_recibo_ult1']\n\n\n%%time\ntrain = pd.read_csv('data/train_ver2.csv', usecols=svd_cols)\nprint(f'train.shape - {train.shape}')\n\n\n%%time\n\n#read one month of data\ntrain1 = train[train.fecha_dato == '2015-01-28'].drop('fecha_dato', axis=1).copy()\nprint(f'train1.shape - {train1.shape} and true.shape - {true.shape}')\n\ntrain1.shape - (625457, 25) and true.shape - (625457, 25)\nCPU times: user 695 ms, sys: 163 ms, total: 858 ms\nWall time: 851 ms\n\n\n\nusers = true['ncodpers'].tolist()\ntrue.drop('ncodpers', axis=1, inplace=True)\n\nitems = true.columns.tolist()\nprint(items)\n\nu = {}\nfor i in range(len(users)):\n    u[users[i]] = i\n\n['ind_ahor_fin_ult1', 'ind_aval_fin_ult1', 'ind_cco_fin_ult1', 'ind_cder_fin_ult1', 'ind_cno_fin_ult1', 'ind_ctju_fin_ult1', 'ind_ctma_fin_ult1', 'ind_ctop_fin_ult1', 'ind_ctpp_fin_ult1', 'ind_deco_fin_ult1', 'ind_deme_fin_ult1', 'ind_dela_fin_ult1', 'ind_ecue_fin_ult1', 'ind_fond_fin_ult1', 'ind_hip_fin_ult1', 'ind_plan_fin_ult1', 'ind_pres_fin_ult1', 'ind_reca_fin_ult1', 'ind_tjcr_fin_ult1', 'ind_valo_fin_ult1', 'ind_viv_fin_ult1', 'ind_nomina_ult1', 'ind_nom_pens_ult1', 'ind_recibo_ult1']\n\n\n\ntrueMat = np.array(true)\n\n\nusers = train['ncodpers'].tolist()\nprint(len(users))\nu = {}\n\n13647309\n\n\n\nfor i in range(len(users)):\n    u[users[i]] = i\n\n\ntrain1.index = train1['ncodpers'].tolist()\ntrain1.drop('ncodpers', axis=1, inplace=True)\ntrain1.head()\n\n\n\n\n\n\n\n\nind_ahor_fin_ult1\nind_aval_fin_ult1\nind_cco_fin_ult1\nind_cder_fin_ult1\nind_cno_fin_ult1\nind_ctju_fin_ult1\nind_ctma_fin_ult1\nind_ctop_fin_ult1\nind_ctpp_fin_ult1\nind_deco_fin_ult1\n...\nind_hip_fin_ult1\nind_plan_fin_ult1\nind_pres_fin_ult1\nind_reca_fin_ult1\nind_tjcr_fin_ult1\nind_valo_fin_ult1\nind_viv_fin_ult1\nind_nomina_ult1\nind_nom_pens_ult1\nind_recibo_ult1\n\n\n\n\n1375586\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0.0\n0.0\n0\n\n\n1050611\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0.0\n0.0\n0\n\n\n1050612\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0.0\n0.0\n0\n\n\n1050613\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n...\n0\n0\n0\n0\n0\n0\n0\n0.0\n0.0\n0\n\n\n1050614\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0.0\n0.0\n0\n\n\n\n\n5 rows × 24 columns\n\n\n\n\n%%time\ntrain1 = train1.reindex(users)\nprint(f'train1.shape{train1.shape}')\ntrain1.head(3)\n\ntrain1.shape(13647309, 24)\nCPU times: user 2.87 s, sys: 6.77 s, total: 9.64 s\nWall time: 9.83 s\n\n\n\n\n\n\n\n\n\nind_ahor_fin_ult1\nind_aval_fin_ult1\nind_cco_fin_ult1\nind_cder_fin_ult1\nind_cno_fin_ult1\nind_ctju_fin_ult1\nind_ctma_fin_ult1\nind_ctop_fin_ult1\nind_ctpp_fin_ult1\nind_deco_fin_ult1\n...\nind_hip_fin_ult1\nind_plan_fin_ult1\nind_pres_fin_ult1\nind_reca_fin_ult1\nind_tjcr_fin_ult1\nind_valo_fin_ult1\nind_viv_fin_ult1\nind_nomina_ult1\nind_nom_pens_ult1\nind_recibo_ult1\n\n\n\n\n1375586\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1050611\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1050612\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n3 rows × 24 columns\n\n\n\n\n\nsvd\n\n%%time\nutilMat = np.array(train1)\nmask = np.isnan(utilMat)\nmask_zero = \n\nCPU times: user 1.37 s, sys: 11.2 s, total: 12.5 s\nWall time: 12.7 s\n\n\n\nnp.where(utilMat)\n\n(array([       0,        1,        2, ..., 13647308, 13647308, 13647308]),\n array([ 2,  2,  2, ..., 21, 22, 23]))\n\n\n\nprint(np.sum(mask))\nprint(utilMat.shape[0] * utilMat.shape[1])\n\n75917862\n327535416\n\n\n\n%%time\nmasked_arr=np.ma.masked_array(utilMat, mask)\nitem_means=np.mean(masked_arr, axis=0)\nprint(f'masked_arr.shape - {masked_arr.shape}')\nprint(f'item_means.shape - {item_means.shape}')\n\nmasked_arr.shape - (13647309, 24)\nitem_means.shape - (24,)\nCPU times: user 2.36 s, sys: 9.37 s, total: 11.7 s\nWall time: 11.9 s\n\n\n\nprint(item_means)\n\n[0.00014086538076379747 3.0805360857621244e-05 0.7842677689708568\n 0.00048544670825167844 0.09478923983051138 0.01233997902565028\n 0.013272341897923052 0.17154113021149453 0.058354032374049944\n 0.0022137895858424282 0.002612161079038354 0.057973781681358506\n 0.0969037465041161 0.02239120557495011 0.00805860610224633\n 0.012010752692274092 0.0034949301815093824 0.060178987730119864\n 0.05746153526288704 0.0327036577694172 0.005214594149384828\n 0.0630621993029027 0.07005819202959135 0.1539579452461892]\n\n\n\nmasked_arr\n\nmasked_array(\n  data=[[0.0, 0.0, 1.0, ..., 0.0, 0.0, 0.0],\n        [0.0, 0.0, 1.0, ..., 0.0, 0.0, 0.0],\n        [0.0, 0.0, 1.0, ..., 0.0, 0.0, 0.0],\n        ...,\n        [0.0, 0.0, 1.0, ..., 0.0, 0.0, 0.0],\n        [0.0, 0.0, 1.0, ..., 0.0, 0.0, 0.0],\n        [--, --, --, ..., --, --, --]],\n  mask=[[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [ True,  True,  True, ...,  True,  True,  True]],\n  fill_value=1e+20)\n\n\n\n%%time\nutilMat2 = masked_arr.filled(item_means)\nprint(f'utilMat2.shape - {utilMat2.shape}')\n\nutilMat2.shape - (13647309, 24)\nCPU times: user 1.73 s, sys: 7.63 s, total: 9.36 s\nWall time: 9.89 s\n\n\n\nutilMat2\n\narray([[0.00000000e+00, 0.00000000e+00, 1.00000000e+00, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 1.00000000e+00, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 1.00000000e+00, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       ...,\n       [0.00000000e+00, 0.00000000e+00, 1.00000000e+00, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 0.00000000e+00, 1.00000000e+00, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [1.40865381e-04, 3.08053609e-05, 7.84267769e-01, ...,\n        6.30621993e-02, 7.00581920e-02, 1.53957945e-01]])\n\n\n\n%%time\nx = np.tile(item_means, (utilMat2.shape[0],1))\nprint(f'x.shape - {x.shape}')\n\nx.shape - (13647309, 24)\nCPU times: user 452 ms, sys: 5.99 s, total: 6.44 s\nWall time: 7.17 s\n\n\n\na = np.array([[1,0],[2,3]])\na\n\narray([[1, 0],\n       [2, 3]])\n\n\n\nmask = np.isnan(a)\nmask\n\narray([[False, False],\n       [False, False]])"
  },
  {
    "objectID": "posts/2022-08-18-pytorch-datapipes.html",
    "href": "posts/2022-08-18-pytorch-datapipes.html",
    "title": "Pytorch Datapipes",
    "section": "",
    "text": "from functools import reduce\nfrom operator import add\nl = [[1], [2,3]]\nreduce(add, l)\n\n[1, 2, 3]"
  },
  {
    "objectID": "posts/2022-10-25-transformers-from-scratch.html",
    "href": "posts/2022-10-25-transformers-from-scratch.html",
    "title": "Transformers from scratch in pytorch",
    "section": "",
    "text": "## Standard Libraries\nimport os\nfrom tracemalloc import Snapshot\nimport numpy as numpy\nimport random\nimport math\nimport json\nfrom functools import partial\nimport logging\nimport sys\n\n# imports for plotting\nimport matplotlib.pyplot as plt\nimport matplotlib_inline\nplt.set_cmap('cividis')\n%matplotlib inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats()\nfrom matplotlib.colors import to_rgb\nimport matplotlib\nmatplotlib.rcParams['lines.linewidth'] = 2.0\nimport seaborn as sns\nsns.reset_orig()\n\nfrom tqdm.notebook import tqdm\n\n# pytorch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision\nfrom torchvision import transforms\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n\n# others\nimport einops\n\n# path to folder where datasets should be downloaded\nDATASET_PATH = \"../data\"\nCHECKPOINT_PATH = \"../saved_models\"\n\n# setting the seed\npl.seed_everything(42)\n\n# Ensure that all operations are deterministirc on GPU\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndevice = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\nprint(\"Device: \", device)\n\nGlobal seed set to 42\n\n\nDevice:  cpu\n\n\n\nWhat is Attention?\nThe attention mechanism describes a weighted average of sequence of elements with the weights dynamically computed based on the input query and the element’s keys.\n As we can see above, Query, Key and Value are all just modified word embedding.\n\n\n\nimage\n\n\n(image from http://jalammar.github.io/illustrated-transformer/) #### Self-attention We use self-attention to modify each embedding of the input word as a combination of modified values from the word embedding and the weights computed. Some intuitions\n\nValues are nothing but modified word embeddings which will be weighted and given as outputs\nQuery is the modified word embedding which will search for relevant word embeddings in the sequence\nKeys are modified word embeddings which will be searched by the query to create the weights\nAs we are using dot product, we are essentially computing the cosine similarity between the query and the key. The higher the cosine similarity, the higher the weight.\n\nIf we look at the softmax scores, clearly the word at its own position will have the highest softmax score, but sometimes it’s useful to attend to another word that is relevant to the current word.\nLets look at the statement The animal didn't cross the street because it was too tired. In this case, softmax for ‘it’ will have a parts of itself, animal (more) and street (less)\nIn addition to attending to other parts of sentence, the softmax can also drown out the irrelvant words by multiplying them with a very small number.\n\n\n\nimage\n\n\n\n\nMulti Head Attention\nAn attention layer outputs a representation of the input sequence, based on the weights learnt for Query, Key and Value. We use multiple heads and combine them using a linear layer to get a better representation of the input sequence.\n\ndef scaled_dot_product(q, k, v, mask=None):\n    d_k = q.size()[-1]\n    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n    attn_logits = attn_logits/math.sqrt(d_k) \n    if mask is not None:\n        attn_logits = attn_logits.masked_fill(mask==0, -9e15)\n    attention = F.softmax(attn_logits, dim=1)\n    values = torch.matmul(attention, v)\n    return values, attention\n\n\nseq_len, d_k = 3, 2\nq = torch.randn(seq_len, d_k)\nk = torch.randn(seq_len, d_k)\nv = torch.randn(seq_len, d_k)\nvalues, attention = scaled_dot_product(q, k, v)\nprint(\"Q\\n\", q.size())\nprint(\"K\\n\", k.size())\nprint(\"V\\n\", v.size())\nprint(\"Values\\n\", values.size())\nprint(\"Attention\\n\", attention.size())\n\nQ\n torch.Size([3, 2])\nK\n torch.Size([3, 2])\nV\n torch.Size([3, 2])\nValues\n torch.Size([3, 2])\nAttention\n torch.Size([3, 3])\n\n\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, input_dim, embed_dim, num_heads, log_level=logging.INFO):\n        super().__init__()\n        assert embed_dim%num_heads == 0, \"Embedding dimension should be zero module with number of heads\"\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim//num_heads\n        # logging.getLogger().setLevel(logging.DEBUG)\n        self.logger = logging.getLogger(self.__class__.__name__)\n        # print(f'log_level - {log_level}')\n        self.logger.setLevel(log_level)\n\n        # stack all weight matrices 1...h together for efficiency\n        # Note that in many implemenations, you see bias=false, which is optional\n        self.qkv_proj = nn.Linear(input_dim, 3*embed_dim) \n        self.o_proj = nn.Linear(embed_dim, embed_dim)\n        self._reset_parameters()\n\n    def _reset_parameters(self):\n        # Orignial Transformer initialization\n        nn.init.xavier_uniform_(self.qkv_proj.weight)\n        self.qkv_proj.bias.data.fill_(0)\n        nn.init.xavier_uniform_(self.o_proj.weight)\n        self.o_proj.bias.data.fill_(0)\n\n    def forward(self, x, mask=None, return_attention=False):\n        batch_size, seq_length, _ = x.size()\n        qkv = self.qkv_proj(x)\n        self.logger.debug(f'x.shape - {x.size()}. qkv_proj.shape - {self.qkv_proj.weight.size()}')\n\n        # separate Q, K, V from the linear output\n        self.logger.debug(f'generated qkv.shape - {qkv.size()}')\n        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)\n        self.logger.debug(f'reshaped qkv.shape - {qkv.size()}')\n        qkv = qkv.permute(0, 2, 1, 3) # [Batch, seqlen, Head, DIms ]\n        self.logger.debug(f'permuted qkv.shape - {qkv.size()}')\n        q, k, v = qkv.chunk(3, dim=-1)\n\n        # determine value outputs\n        values, attention = scaled_dot_product(q, k, v, mask=mask)\n        self.logger.debug(f'values.shape - {values.size()}. attension.shape - {attention.size()}')\n        values = values.permute(0, 2, 1, 3) # [Batch, seqlen, Head, dims]\n        self.logger.debug(f'values.shape after permute - {values.size()}')\n        values = values.reshape(batch_size, seq_length, self.embed_dim)\n        self.logger.debug(f'values.shape after reshape - {values.size()}')\n\n        o = self.o_proj(values)\n\n        if return_attention:\n            return o, attention\n        else:\n            return o\n\ninput_dim = 64\nembed_dim = 16\nbatch_size = 3\nseq_len = 10\nmulti_attention = MultiHeadAttention(input_dim, embed_dim, num_heads=4, log_level=logging.DEBUG)\nx = torch.randn(batch_size, seq_len, input_dim)\natt = multi_attention(x)\n\nDEBUG:MultiHeadAttention:x.shape - torch.Size([3, 10, 64]). qkv_proj.shape - torch.Size([48, 64])\nDEBUG:MultiHeadAttention:generated qkv.shape - torch.Size([3, 10, 48])\nDEBUG:MultiHeadAttention:reshaped qkv.shape - torch.Size([3, 10, 4, 12])\nDEBUG:MultiHeadAttention:permuted qkv.shape - torch.Size([3, 4, 10, 12])\nDEBUG:MultiHeadAttention:values.shape - torch.Size([3, 4, 10, 4]). attension.shape - torch.Size([3, 4, 10, 10])\nDEBUG:MultiHeadAttention:values.shape after permute - torch.Size([3, 10, 4, 4])\nDEBUG:MultiHeadAttention:values.shape after reshape - torch.Size([3, 10, 16])\n\n\n\nclass EncoderBlock(nn.Module):\n    def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.0):\n        super().__init__()\n\n        # attention layer\n        self.self_attn = MultiHeadAttention(input_dim, input_dim, num_heads)\n\n        # two layer MLP\n        self.linear_net = nn.Sequential(\n            nn.Linear(input_dim, dim_feedforward),\n            nn.Dropout(dropout),\n            nn.ReLU(inplace=True),\n            nn.Linear(dim_feedforward, input_dim)\n        )\n\n        # layers to apply inbetween main layers\n        self.norm1 = nn.LayerNorm(input_dim)\n        self.norm2 = nn.LayerNorm(input_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask=None):\n        # Attention part\n        attn_out = self.self_attn(x, mask=mask)\n        x = x + self.dropout(attn_out)\n        x = self.norm1(x)\n\n        # MLP part\n        linear_out = self.linear_net(x)\n        x = x + self.dropout(linear_out)\n        x = self.norm2(x)\n\n        return x\n\nclass TransformerEncoder(nn.Module):\n    def __init__(self, num_layers, **block_args):\n        super().__init__()\n        self.layers = nn.ModuleList([EncoderBlock(**block_args) for _ in range(num_layers)])\n\n    def forward(self, x, mask=None):\n        for l in self.layers:\n            x = l(x, mask=mask)\n        return x\n    \n    def get_attention_maps(self, x, mask=None):\n        attention_maps = []\n        for l in self.layers:\n            _, attn_map = l.self_attn(x, mask=mask, return_attention=True)\n            attention_maps.append(attn_map)\n            x = l(x)\n        return attention_maps\n\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super().__init__()\n\n        # create a matrix of seq_len, hidden_dim representing the positinal encoding for \n        pe = torch.zeros(max_len, d_model)\n\n        # position is index of the word in the sequence\n        position = torch.arange(0,  max_len, dtype=torch.float).unsqueeze(1)\n        \n        div_term = torch.exp(torch.arange(0, d_model, 2)) * -math.log(10000)/d_model\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n\n        # register_buffer =&gt; tensor which is not a parameter, but shold be a part of the modules state\n        # used for tensors that need to be on the same device as the module\n        # persistent=False tell pytorch not to add the buffer to the state_dict\n        self.register_buffer('pe', pe, persistent=False)\n\n    def forward(self, x):\n        x = x + self.pe[:, :x.size(1)]\n        return x\n        \n        \n\n\n\n\n\\[\nf(n) =\n\\begin{cases}\nn/2,  & \\text{if $n$ is even} \\\\\n3n+1, & \\text{if $n$ is odd}\n\\end{cases}\n\\]\n\n\nPositional Encoding\nWe add a fixed signal (not trainable) to each word based on its position. The dimension of the PE signal is same is same as the word dimension.\n\\[\nPE_{(pos, i)} =\n\\begin{cases}\n    sin(\\frac{pos}{10000^{i/d_{model}}}),  & \\text{if $i$ mod 2 = 0}\\\\\n    cos(\\frac{pos}{10000^{i-1/d_{model}}}),  & \\text{otherwise}\n\\end{cases}\n\\]\nHere pos is the word position and i is the embedding position.\nFor a deeper intuition, look at - https://kazemnejad.com/blog/transformer_architecture_positional_encoding/ - https://towardsdatascience.com/master-positional-encoding-part-i-63c05d90a0c3"
  },
  {
    "objectID": "posts/2023-01-30-finetune-nlp-pytorch.html",
    "href": "posts/2023-01-30-finetune-nlp-pytorch.html",
    "title": "5 The datasets library",
    "section": "",
    "text": "Repeating steps from https://huggingface.co/course/chapter3/4?fw=pt\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\nfrom datasets import load_dataset\nfrom transformers import Trainer, TrainingArguments\n\n\ncheckpoint = 'bert-base-uncased'\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint)\nraw_datasets = load_dataset('glue', 'mrpc')\n\ndef tokenize_function(examples):\n    return tokenizer(examples['sentence1'], examples['sentence2'], truncation=True)\n\ntokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\ndata_collator = DataCollatorWithPadding(tokenizer)\n\n# modify dataset to be used with pytorch model\ntokenized_datasets = tokenized_datasets.remove_columns(['sentence1', 'sentence2', 'idx'])\ntokenized_datasets = tokenized_datasets.rename_column('label', 'labels')\ntokenized_datasets.set_format('torch')\nprint(f\"dataset column names - {tokenized_datasets['train'].column_names}\")\n\n\n# create dataloaders\nfrom torch.utils.data import DataLoader\ntrain_dataloader = DataLoader(tokenized_datasets['train'], shuffle=True, batch_size=8,collate_fn=data_collator)\neval_dataloader = DataLoader(tokenized_datasets['validation'], batch_size=8, collate_fn=data_collator)\n\nbatch = next(iter(train_dataloader))\nprint({k:v.shape for k,v in batch.items()})\n\n# load model\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n# outputs = model(**batch)\n# print(f\"loss - {outputs.loss}. logits.shape - {outputs.logits.shape}\")\n\n# optimizer\nfrom transformers import AdamW\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\n# scheduler\nfrom transformers import get_scheduler\nnum_epochs = 3\nnum_training_steps = num_epochs * len(train_dataloader)\nlr_scheduler = get_scheduler('linear', optimizer=optimizer,num_warmup_steps=0,num_training_steps=num_training_steps)\nprint(f'num_training_steps - {num_training_steps}')\n\n# detect device\nimport torch\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n# training loop\nfrom tqdm.auto import tqdm\nprogress_bar = tqdm(range(num_training_steps))\nmodel.train()\nfor epoch in range(num_epochs):\n    for batch in train_dataloader:\n        batch = {k:v.to(device) for k,v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        progress_bar.update(1)\n\n# evaluation loop\nimport evaluate\nmetric = evaluate.load('glue','mrpc')\nfor batch in eval_dataloader:\n    batch = {k:v.to(device) for k, v in batch.items()}\n    with torch.no_grad():\n        outputs = model(**batch)\n    logits = outputs.logits\n    predictions = torch.argmax(logits, dim=-1)\n    metric.add_batch(predictions=predictions, references=batch['labels'])\nmetric.compute()\n\nSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nFound cached dataset glue (/Users/achinta/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\nLoading cached processed dataset at /Users/achinta/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-b8ca0e77d9b1a107.arrow\nLoading cached processed dataset at /Users/achinta/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-2656b493beabb728.arrow\nLoading cached processed dataset at /Users/achinta/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-e8576c3577f28c58.arrow\nYou're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/Users/achinta/miniforge3/envs/ml/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n\n\n\n\n\ndataset column names - ['labels', 'input_ids', 'token_type_ids', 'attention_mask']\n{'labels': torch.Size([8]), 'input_ids': torch.Size([8, 72]), 'token_type_ids': torch.Size([8, 72]), 'attention_mask': torch.Size([8, 72])}\nnum_training_steps - 1377\n\n\n\n\n\n{'accuracy': 0.8578431372549019, 'f1': 0.8999999999999999}\n\n\nThe ‘accelerate’ version of the code can be seen in https://huggingface.co/course/chapter3/4?fw=pt\n\n\n\n{'labels': torch.Size([8]),\n 'input_ids': torch.Size([8, 75]),\n 'token_type_ids': torch.Size([8, 75]),\n 'attention_mask': torch.Size([8, 75])}\n\n\n\n3.1 time to slice and dice\n\n\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n-rw-r--r--  1 achinta  staff    80M Oct  2  2018 drugsComTrain_raw.tsv\n-rw-r--r--  1 achinta  staff    27M Oct  2  2018 drugsComTest_raw.tsv\nCPU times: user 67.3 ms, sys: 37.3 ms, total: 105 ms\nWall time: 1.37 s\n\n\nUsing custom data configuration default-4eaca5caac99961c\nFound cached dataset csv (/Users/achinta/.cache/huggingface/datasets/csv/default-4eaca5caac99961c/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)"
  },
  {
    "objectID": "posts/2022-01-06-linear-regression.html",
    "href": "posts/2022-01-06-linear-regression.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Matrix Decompositions\nMatrix decomposition is the expression of a matrix as a product of matrices. Just as factorizing a number can tell us important properties of a number, so does matrix decomposition can reveal important properties.\nHere are five important matrix factorizations\n\n\\(A = LU\\)\nThis comes from elimination. Matrix L is lower triangular and U is upper triangular.\n\n\n\\(A = QR\\)\nComes from orthogonalizing the columns \\(a_1\\) to \\(a_n\\), as in ‘Gram-Schmidt’\n\n\n\\(S = Q{\\Lambda}Q^T\\)\nS is a symmetric matrix. Q has orthonormal eigen vectors as its columns. The corresponding eigen values are the digagonal of \\(\\Lambda\\)\n\nimport numpy as np\nfrom scipy import linalg \nimport random\n\n# create a symmetric matrix\na = np.random.randint(1, 10, size=(3,3))\ns = np.tril(a, -1).T + np.tril(a)\nprint(f'symmetric matrix: \\n\\n{s}')\n\n# eigen decomposition\nw, q = linalg.eigh(s)\nprint(f'\\neigen values - {w}')\n\nsymmetric matrix: \n\n[[5 6 1]\n [6 5 5]\n [1 5 3]]\n\neigen values - [-2.78571018  2.86068379 12.92502638]\n\n\n\n# lets reconstruct the matrix\nnp.linalg.multi_dot([q, np.diag(w), np.transpose(q)])\n\narray([[6., 5., 4.],\n       [5., 3., 5.],\n       [4., 5., 4.]])\n\n\n\n# Check that the columns of the matrix are orthogonal\n# select two columns randomly and check the dot prodct\ncol_idxs = random.sample(range(0, s.shape[0]), k=2)\nassert np.isclose(np.dot(q[col_idxs[0]], q[col_idxs[1]]), 0)\n\n\n# check the Euclidean norm of any of columns. It should be 1 as they are orthonormal\ncol_idx = random.choice(range(0, s.shape[0]))\nnp.linalg.norm(q[col_idx], ord=2)\n\n0.9999999999999999"
  },
  {
    "objectID": "posts/2022-11-01-text-from-invoices.html",
    "href": "posts/2022-11-01-text-from-invoices.html",
    "title": "notes",
    "section": "",
    "text": "from transformers import AutoModelForTokenClassification, AutoProcessor\nfrom datasets import ClassLabel, Features, Sequence, Value, Array2D, Array3D, load_metric\nimport torch\nfrom transformers import LayoutLMv3ForTokenClassification\nfrom transformers.data.data_collator import default_data_collator\nfrom PIL import Image, ImageDraw, ImageFont\n\n\nprocesser = AutoProcessor.from_pretrained(\"microsoft/layoutlmv3-base\", apply_ocr=True)\nmodel = AutoModelForTokenClassification.from_pretrained(\"microsoft/layoutlmv3-base\")\n\nSome weights of LayoutLMv3ForTokenClassification were not initialized from the model checkpoint at microsoft/layoutlmv3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\ndigits = [5, 7, 2]\nfor idx in range(len(digits), 0, -1):\n    print(idx)\n    \n\n3\n2\n1\n\n\n\nfrom transformers import LayoutLMv2Model\nimport transformers\ntransformers.utils.import_utils._detectron2_available\n\nTrue\n\n\n\ntransformers.__version__\n\n'4.23.1'"
  },
  {
    "objectID": "posts/2022-01-16-movie-recommender-pytorch.html",
    "href": "posts/2022-01-16-movie-recommender-pytorch.html",
    "title": "Movie Recommender using pytorch",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport pytorch_lightning as pl\nimport numpy as np\nimport pandas as pd\n\n\n# !wget https://files.grouplens.org/datasets/movielens/ml-100k.zip\n# !unzip ml-100k.zip\n\n\n# read movie ratings\nratings_cols = ['user_id', 'item_id', 'rating', 'timestamp']\nratings_train_df = pd.read_csv('ml-100k/u1.base', sep='\\t', header=None)\nratings_train_df.columns = ratings_cols\nprint(f'ratings_train_df.shape \\t - {ratings_train_df.shape}')\n\nratings_test_df = pd.read_csv('ml-100k/u1.test', sep='\\t', header=None)\nratings_test_df.columns = ratings_cols\nprint(f'ratings_test_shape \\t - {ratings_test_df.shape}')\n\nratings_train_df.head(2)\n\nratings_train_df.shape   - (80000, 4)\nratings_test_shape   - (20000, 4)\n\n\n\n\n\n\n\n\n\nuser_id\nitem_id\nrating\ntimestamp\n\n\n\n\n0\n1\n1\n5\n874965758\n\n\n1\n1\n2\n3\n876893171\n\n\n\n\n\n\n\n\n# read movie data\nmovies_df = pd.read_csv('ml-100k/u.item', sep='|', header=None)\nprint(f'users_df.shape - {movies_df.shape}')\nmovie_cols = ['movie_id','title', 'release_date', 'video_release_date', 'imdb_url', 'unknown',\n             'action', 'adventure', 'animation', 'childrens', 'comedy', 'crime', 'documentary', \n             'drama', 'fantasy','film-noir', 'horror', 'musical','mystery', 'romance', \n             'scifi', 'thriller', 'war', 'western']\nmovies_df.columns = movie_cols\nmovies_df.head(2)\n\nusers_df.shape - (1682, 24)\n\n\n\n\n\n\n\n\n\nmovie_id\ntitle\nrelease_date\nvideo_release_date\nimdb_url\nunknown\naction\nadventure\nanimation\nchildrens\n...\nfantasy\nfilm-noir\nhorror\nmusical\nmystery\nromance\nscifi\nthriller\nwar\nwestern\n\n\n\n\n0\n1\nToy Story (1995)\n01-Jan-1995\nNaN\nhttp://us.imdb.com/M/title-exact?Toy%20Story%2...\n0\n0\n0\n1\n1\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n2\nGoldenEye (1995)\n01-Jan-1995\nNaN\nhttp://us.imdb.com/M/title-exact?GoldenEye%20(...\n0\n1\n1\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n\n\n2 rows × 24 columns\n\n\n\n\n# read user data\nusers_df = pd.read_csv('ml-100k/u.user', sep='|', header=None)\nuser_cols = ['user_id', 'age', 'gender', 'occupation', 'zip_code']\nusers_df.columns = user_cols\nusers_df.head(2)\n\n\n\n\n\n\n\n\nuser_id\nage\ngender\noccupation\nzip_code\n\n\n\n\n0\n1\n24\nM\ntechnician\n85711\n\n\n1\n2\n53\nF\nother\n94043\n\n\n\n\n\n\n\n\nReferences\nhttps://yonigottesman.github.io/recsys/pytorch/elasticsearch/2020/02/18/fm-torch-to-recsys.html"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "notes",
    "section": "",
    "text": ":::{#quarto-listing-pipeline .hidden} \\(e = mC^2\\)\n:::{.hidden render-id=“pipeline-listing-listing”}\n:::{.list .quarto-listing-default}\n\n\n  \n\n\n\n\nPurpose\n\n\n\n\n\n\n\n\n\n\n\n\nApr 19, 2023\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 23, 2023\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 19, 2023\n\n\n\n\n\n\n  \n\n\n\n\nResults\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 15, 2023\n\n\n\n\n\n\n  \n\n\n\n\n5 The datasets library\n\n\n\n\n\n\n\n\n\n\n\n\nJan 30, 2023\n\n\n\n\n\n\n  \n\n\n\n\n\nTransformer Models\n\n\n\n\n\n\n\n\n\n\n\n\nJan 23, 2023\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2022\n\n\n\n\n\n\n  \n\n\n\n\nTransformers from scratch in pytorch\n\n\n\n\n\n\n\nfastpages\n\n\njupyter\n\n\n\n\nNotes from https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html\n\n\n\n\n\n\nOct 25, 2022\n\n\n\n\n\n\n  \n\n\n\n\nPytorch Datapipes\n\n\n\n\n\nUnderstanding by doing\n\n\n\n\n\n\nAug 18, 2022\n\n\n\n\n\n\n  \n\n\n\n\nProperties of Eigen Values and Vectors\n\n\n\n\n\n\n\n\n\n\n\n\nJun 29, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 26, 2022\n\n\n\n\n\n\n  \n\n\n\n\nPassenger dataset\n\n\n\n\n\n\n\n\n\n\n\n\nMay 13, 2022\n\n\n\n\n\n\n  \n\n\n\n\nSatender Product Recommendation\n\n\n\n\n\nFrom SVD to Deep Learning\n\n\n\n\n\n\nMar 7, 2022\n\n\n\n\n\n\n  \n\n\n\n\nMovie Recommender using pytorch\n\n\n\n\n\nSimple implementation\n\n\n\n\n\n\nJan 16, 2022\n\n\n\n\n\n\n  \n\n\n\n\nSklearn - Adult Income Classificat\n\n\n\n\n\nBasics\n\n\n\n\n\n\nJan 10, 2022\n\n\n\n\n\n\n  \n\n\n\n\nLinear Regression\n\n\n\n\n\nBasics\n\n\n\n\n\n\nJan 6, 2022\n\n\n\n\n\n\n  \n\n\n\n\nVariational Inference\n\n\n\n\n\nUnder the hood\n\n\n\n\n\n\nDec 28, 2021\n\n\n\n\n\n\n  \n\n\n\n\nMIT 18.065 Linear Algebra, Lecture 1\n\n\n\n\n\nUnderstanding column spaces\n\n\n\n\n\n\nDec 23, 2021\n\n\n\n\n\n\n  \n\n\n\n\nsklearn intro\n\n\n\n\n\n\n\nfastpages\n\n\njupyter\n\n\n\n\nClassification, Clustering and Regression\n\n\n\n\n\n\nNov 28, 2021\n\n\n\n\n\n\n  \n\n\n\n\nHands - on ML. Chapter 1\n\n\n\n\n\nClassification, Clustering and Regression\n\n\n\n\n\n\nNov 22, 2021\n\n\n\n\n\n\n  \n\n\n\n\nFastai Linear Algebra - chapter 1\n\n\n\n\n\nImage background removal with SGD\n\n\n\n\n\n\nNov 11, 2021\n\n\n\n\n:::\n\n\nNo matching items\n\n:::\n:::"
  }
]